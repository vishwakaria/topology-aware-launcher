{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Distributed data parallel MNIST training with PyTorch and SageMaker distributed\n",
    "\n",
    "## Background\n",
    "[Amazon SageMaker's distributed library](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html) can be used to train deep learning models faster and cheaper. The [data parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html) feature in this library is a distributed data parallel training framework for PyTorch, TensorFlow, and MXNet. This notebook demonstrates how to use the SageMaker distributed data library to train a PyTorch model using the MNIST dataset.\n",
    "\n",
    "This notebook example shows how to use `smdistributed.dataparallel` with PyTorch in SageMaker using MNIST dataset.\n",
    "\n",
    "For more information:\n",
    "\n",
    "1. [SageMaker distributed data parallel PyTorch API Specification](https://sagemaker.readthedocs.io/en/stable/api/training/smd_data_parallel_pytorch.html)\n",
    "1. [Getting started with SageMaker distributed data parallel](https://sagemaker.readthedocs.io/en/stable/api/training/smd_data_parallel.html)\n",
    "1. [PyTorch in SageMaker](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "\n",
    "### Dataset\n",
    "This example uses the MNIST dataset. MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This example requires SageMaker Python SDK v2.**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (2.150.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.26.114)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (5.4.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (4.11.4)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.114 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.114)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from jsonschema->sagemaker) (0.18.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from jsonschema->sagemaker) (65.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.114->boto3<2.0,>=1.26.28->sagemaker) (1.26.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker role\n",
    "\n",
    "The following code cell defines `role` which is the IAM role ARN used to create and run SageMaker training and hosting jobs. This is the same IAM role used to create this SageMaker Notebook instance. \n",
    "\n",
    "`role` must have permission to create a SageMaker training job and launch an endpoint to host a model. For granular policies you can use to grant these permissions, see [Amazon SageMaker Roles](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amazon Resource Name (ARN) of the role used for this demo is: arn:aws:iam::570106654206:role/ziyi-dev\n",
      "The name of the role used for this demo is: ziyi-dev\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "role_name = role.split([\"/\"][-1])\n",
    "print(f\"The Amazon Resource Name (ARN) of the role used for this demo is: {role}\")\n",
    "print(f\"The name of the role used for this demo is: {role_name[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the role above has required permissions:\n",
    "\n",
    "1. Go to the IAM console: https://console.aws.amazon.com/iam/home.\n",
    "2. Select **Roles**.\n",
    "3. Enter the role name in the search box to search for that role. \n",
    "4. Select the role.\n",
    "5. Use the **Permissions** tab to verify this role has required permissions attached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training with SageMaker distributed data parallel\n",
    "\n",
    "### Training script\n",
    "\n",
    "The MNIST dataset is downloaded using the `torchvision.datasets` PyTorch module; you can see how this is implemented in the `train_pytorch_smdataparallel_mnist.py` training script that is printed out in the next cell.\n",
    "\n",
    "The training script provides the code you need for distributed data parallel (DDP) training using SageMaker's distributed data parallel library (`smdistributed.dataparallel`). The training script is very similar to a PyTorch training script you might run outside SageMaker, but modified to run with the `smdistributed.dataparallel` library. This library's PyTorch client provides an alternative to PyTorch's native DDP.\n",
    "\n",
    "For details about how to use `smdistributed.dataparallel`'s DDP in your native PyTorch script, see the [Modify a PyTorch Training Script Using SMD Data Parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html#data-parallel-modify-sdp-pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize code/train_pytorch_smdataparallel_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator function options\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distribution strategy. You're also passing in the training script you reviewed in the previous cell to this estimator.\n",
    "\n",
    "**Instance types**\n",
    "\n",
    "`smdistributed.dataparallel` supports model training on SageMaker with the following instance types only.  For best performance, it is recommended you use an instance type that supports Amazon Elastic Fabric Adapter (ml.p3dn.24xlarge and ml.p4d.24xlarge).\n",
    "\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [Recommended]\n",
    "1. ml.p4d.24xlarge [Recommended]\n",
    "\n",
    "**Instance count**\n",
    "\n",
    "To get the best performance and the most out of `smdistributed.dataparallel`, you should use at least 2 instances, but you can also use 1 for testing this example.\n",
    "\n",
    "**Distribution strategy**\n",
    "\n",
    "Note that to use DDP mode, you update the `distribution` strategy, and set it to use `smdistributed dataparallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "\n",
    "group1 = InstanceGroup(\"spine1\", \"ml.c5n.2xlarge\", 4)\n",
    "group2 = InstanceGroup(\"spine2\", \"ml.c5n.2xlarge\", 4)  \n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"pytorch-smdataparallel-mnist\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"entry_point.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    #instance_count=4,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    #instance_type=\"ml.c5n.2xlarge\",\n",
    "    instance_groups = [group1, group2],\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    debugger_hook_config=False,\n",
    "    #keep_alive_period_in_seconds=1800,\n",
    "    hyperparameters = {'pp-degree': 4, 'dp-degree': 2,\n",
    "                       'optimize-for-pp': 'False', 'dp-major': 'True', 'entry-point': 'hello.py',\n",
    "                       'flag1' : 'flag1_val', 'flag2' : 'flag2_val', 'flag3': None}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-04-25 00:58:47 Starting - Starting the training job...\n",
      "2023-04-25 00:59:05 Starting - Preparing the instances for training......\n",
      "2023-04-25 01:00:06 Downloading - Downloading input data...\n",
      "2023-04-25 01:00:26 Training - Downloading the training image...\n",
      "2023-04-25 01:01:17 Training - Training image download completed. Training in progress...\n",
      "2023-04-25 01:01:49 Uploading - Uploading generated training model\n",
      "2023-04-25 01:01:49 Completed - Training job completed\n",
      "\u001b[33mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[33mbash: no job control in this shell\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,364 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,366 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,368 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,378 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,380 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,586 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,588 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,602 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,604 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,617 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,619 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:31,629 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[33mTraining Env:\u001b[0m\n",
      "\u001b[33m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-5\",\n",
      "    \"current_instance_group\": \"spine1\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-5\",\n",
      "        \"algo-8\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-5\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[33m}\u001b[0m\n",
      "\u001b[33mEnvironment variables:\u001b[0m\n",
      "\u001b[33mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[33mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[33mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[33mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[33mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine1\",\"current_host\":\"algo-5\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[33mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[33mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[33mSM_CURRENT_HOST=algo-5\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP=spine1\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[33mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[33mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[33mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[33mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[33mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[33mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[33mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[33mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[33mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[33mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[33mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[33mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-5\",\"current_instance_group\":\"spine1\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine1\",\"current_host\":\"algo-5\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[33mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[33mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[33mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[33mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[33mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[33mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[33mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[33mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[33mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[33mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[33mInvoking script with the following command:\u001b[0m\n",
      "\u001b[33m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:32,098 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[33mIn custom launcher\u001b[0m\n",
      "\u001b[33mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[33mLaunching job with command: torchrun --nnodes=8 --node_rank=4 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[33mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[33m*****************************************\u001b[0m\n",
      "\u001b[33mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[33m*****************************************\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:30,905 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:30,907 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:30,908 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:30,919 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:30,921 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,136 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,138 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,151 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,153 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,166 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,168 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,179 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"spine1\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-5\",\n",
      "        \"algo-8\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine1\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=spine1\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"current_instance_group\":\"spine1\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine1\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[35mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[35mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[35mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[35mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[35mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[35mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,636 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mIn custom launcher\u001b[0m\n",
      "\u001b[35mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[35mLaunching job with command: torchrun --nnodes=8 --node_rank=2 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,337 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,338 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,340 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,350 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,352 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,559 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,562 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,574 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,576 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,589 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,591 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,601 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-3\",\n",
      "    \"current_instance_group\": \"spine2\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-3\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine2\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-3\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=spine2\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-3\",\"current_instance_group\":\"spine2\",\"current_instance_group_hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine2\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[32mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[32mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[32mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[32mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[32mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[32mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[32mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:32,058 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mIn custom launcher\u001b[0m\n",
      "\u001b[32mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[32mLaunching job with command: torchrun --nnodes=8 --node_rank=1 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[32mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,356 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,358 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,360 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,369 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,372 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,577 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,579 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,592 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,594 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,607 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,609 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,619 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"spine1\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-5\",\n",
      "        \"algo-8\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine1\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=spine1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"spine1\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine1\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[34mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[34mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[34mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[34mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[34mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[34mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:32,084 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mIn custom launcher\u001b[0m\n",
      "\u001b[34mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[34mLaunching job with command: torchrun --nnodes=8 --node_rank=0 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[36mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[36mbash: no job control in this shell\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,321 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,322 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,324 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,334 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,336 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,554 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,556 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,569 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,571 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,585 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,587 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:31,597 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[36mTraining Env:\u001b[0m\n",
      "\u001b[36m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-4\",\n",
      "    \"current_instance_group\": \"spine2\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-4\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[36m}\u001b[0m\n",
      "\u001b[36mEnvironment variables:\u001b[0m\n",
      "\u001b[36mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[36mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[36mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[36mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[36mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine2\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[36mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[36mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[36mSM_CURRENT_HOST=algo-4\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP=spine2\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[36mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[36mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[36mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[36mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[36mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[36mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[36mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[36mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[36mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[36mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[36mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[36mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-4\",\"current_instance_group\":\"spine2\",\"current_instance_group_hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine2\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[36mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[36mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[36mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[36mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[36mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[36mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[36mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[36mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[36mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[36mInvoking script with the following command:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:32,064 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[36mIn custom launcher\u001b[0m\n",
      "\u001b[36mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[36mLaunching job with command: torchrun --nnodes=8 --node_rank=3 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[36mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[36m*****************************************\u001b[0m\n",
      "\u001b[36mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[36m*****************************************\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,362 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,364 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,366 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,376 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,378 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,584 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,586 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,598 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,600 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,614 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,616 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:31,626 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-8\",\n",
      "    \"current_instance_group\": \"spine1\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-5\",\n",
      "        \"algo-8\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-8\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine1\",\"current_host\":\"algo-8\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-8\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=spine1\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-8\",\"current_instance_group\":\"spine1\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine1\",\"current_host\":\"algo-8\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[32mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[32mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[32mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[32mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[32mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[32mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[32mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:32,091 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mIn custom launcher\u001b[0m\n",
      "\u001b[32mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[32mLaunching job with command: torchrun --nnodes=8 --node_rank=6 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[32mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,413 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,415 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,416 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,426 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,428 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,633 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,635 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,647 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,650 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,663 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,666 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:31,676 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-7\",\n",
      "    \"current_instance_group\": \"spine2\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-7\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine2\",\"current_host\":\"algo-7\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-7\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=spine2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-7\",\"current_instance_group\":\"spine2\",\"current_instance_group_hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine2\",\"current_host\":\"algo-7\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[35mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[35mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[35mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[35mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[35mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[35mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:32,134 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mIn custom launcher\u001b[0m\n",
      "\u001b[35mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[35mLaunching job with command: torchrun --nnodes=8 --node_rank=7 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,298 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,299 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,301 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,311 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,313 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,517 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,520 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,532 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,534 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,547 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,549 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:31,560 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-6\",\n",
      "    \"current_instance_group\": \"spine2\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"hello.py\",\n",
      "        \"flag1\": \"flag1_val\",\n",
      "        \"flag2\": \"flag2_val\",\n",
      "        \"flag3\": null,\n",
      "        \"optimize-for-pp\": \"True\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"spine1\",\n",
      "        \"spine2\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"spine1\": {\n",
      "            \"instance_group_name\": \"spine1\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-5\",\n",
      "                \"algo-8\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        },\n",
      "        \"spine2\": {\n",
      "            \"instance_group_name\": \"spine2\",\n",
      "            \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-6\",\n",
      "                \"algo-7\",\n",
      "                \"algo-3\",\n",
      "                \"algo-4\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": true,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-6\",\n",
      "        \"current_instance_type\": \"ml.c5n.2xlarge\",\n",
      "        \"current_group_name\": \"spine2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"spine1\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-5\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"instance_group_name\": \"spine2\",\n",
      "                \"instance_type\": \"ml.c5n.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-6\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-4\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"spine2\",\"current_host\":\"algo-6\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-6\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5n.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=spine2\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"spine1\",\"spine2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=true\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-6\",\"current_instance_group\":\"spine2\",\"current_instance_group_hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"current_instance_type\":\"ml.c5n.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"hello.py\",\"flag1\":\"flag1_val\",\"flag2\":\"flag2_val\",\"flag3\":null,\"optimize-for-pp\":\"True\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"spine1\",\"spine2\"],\"instance_groups_dict\":{\"spine1\":{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},\"spine2\":{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}},\"is_hetero\":true,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/pytorch-smdataparallel-mnist-2023-04-25-00-58-47-578/source/sourcedir.tar.gz\",\"module_name\":\"entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"spine2\",\"current_host\":\"algo-6\",\"current_instance_type\":\"ml.c5n.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-5\",\"algo-8\",\"algo-1\"],\"instance_group_name\":\"spine1\",\"instance_type\":\"ml.c5n.2xlarge\"},{\"hosts\":[\"algo-6\",\"algo-7\",\"algo-3\",\"algo-4\"],\"instance_group_name\":\"spine2\",\"instance_type\":\"ml.c5n.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"hello.py\",\"--flag1\",\"flag1_val\",\"--flag2\",\"flag2_val\",\"--flag3\",\"\",\"--optimize-for-pp\",\"True\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[34mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[34mSM_HP_ENTRY-POINT=hello.py\u001b[0m\n",
      "\u001b[34mSM_HP_FLAG1=flag1_val\u001b[0m\n",
      "\u001b[34mSM_HP_FLAG2=flag2_val\u001b[0m\n",
      "\u001b[34mSM_HP_FLAG3=\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE-FOR-PP=True\u001b[0m\n",
      "\u001b[34mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 entry_point.py --dp-degree 2 --dp-major True --entry-point hello.py --flag1 flag1_val --flag2 flag2_val --flag3  --optimize-for-pp True --pp-degree 4\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:32,011 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mIn custom launcher\u001b[0m\n",
      "\u001b[34mranking is ['algo-1', 'algo-3', 'algo-2', 'algo-4', 'algo-5', 'algo-6', 'algo-8', 'algo-7']\u001b[0m\n",
      "\u001b[34mLaunching job with command: torchrun --nnodes=8 --node_rank=5 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 hello.py --flag1 flag1_val --flag2 flag2_val --flag3\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[34marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[33marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[35marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[36marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32marguments are ['hello.py', '--flag1', 'flag1_val', '--flag2', 'flag2_val', '--flag3']\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 49\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 51\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 53\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 50\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 54\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 48\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 52\u001b[0m\n",
      "\u001b[32mHello from host algo-8 rank 55\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 61\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 58\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 59\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 62\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 57\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 60\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 63\u001b[0m\n",
      "\u001b[35mHello from host algo-7 rank 56\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 0\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 4\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 5\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 7\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 6\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 1\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 3\u001b[0m\n",
      "\u001b[34mHello from host algo-1 rank 2\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 41\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 42\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 40\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 47\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 45\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 43\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 44\u001b[0m\n",
      "\u001b[34mHello from host algo-6 rank 46\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 38\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 34\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 33\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 37\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 35\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 39\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 32\u001b[0m\n",
      "\u001b[33mHello from host algo-5 rank 36\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 12\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 15\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 8\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 11\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 14\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 9\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 13\u001b[0m\n",
      "\u001b[32mHello from host algo-3 rank 10\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 21Hello from host algo-2 rank 19\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 22\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 18\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 17\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 20\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 16\u001b[0m\n",
      "\u001b[35mHello from host algo-2 rank 23\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 25\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 26\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 24Hello from host algo-4 rank 30\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 31\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 28\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 27\u001b[0m\n",
      "\u001b[36mHello from host algo-4 rank 29\u001b[0m\n",
      "\u001b[33mJob finished!\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:38,988 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:38,988 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[33m2023-04-24 21:01:38,989 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32mJob finished!\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:38,990 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:38,990 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:38,991 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mERROR:torch.distributed.elastic.agent.server.api:Error waiting on exit barrier. Elapsed: 0.005535125732421875 seconds\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py\", line 906, in _exit_barrier\n",
      "    store_util.barrier(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py\", line 67, in barrier\n",
      "    synchronize(store, data, rank, world_size, key_prefix, barrier_timeout)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py\", line 53, in synchronize\n",
      "    agent_data = get_all(store, key_prefix, world_size)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py\", line 31, in get_all\n",
      "    data = store.get(f\"{prefix}{idx}\")\u001b[0m\n",
      "\u001b[35mRuntimeError: Connection reset by peer\u001b[0m\n",
      "\u001b[35mJob finished!\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:38,987 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:38,987 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:38,987 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[36mJob finished!\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:38,989 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:38,989 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[36m2023-04-24 21:01:38,989 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32mJob finished!\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:38,985 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:38,985 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-04-24 21:01:38,986 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35mJob finished!\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:38,993 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:38,993 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-24 21:01:38,994 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mJob finished!\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:38,983 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:38,983 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:38,983 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mJob finished!\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:38,983 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:38,983 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-24 21:01:38,984 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 816\n",
      "Billable seconds: 816\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you have a trained model, you can deploy an endpoint to host the model. After you deploy the endpoint, you can then test it with inference requests. The following cell will store the model_data variable to be used with the inference notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "print(\"Storing {} as model_data\".format(model_data))\n",
    "%store model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: topology-inference-mpi-2023-04-27-05-00-26-292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 05:00:53 Starting - Starting the training job...\n",
      "2023-04-27 05:01:08 Downloading - Downloading input data\n",
      "2023-04-27 05:01:08 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,843 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,904 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,913 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,915 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,849 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,911 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,920 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,922 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,859 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,921 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,930 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,932 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[36mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[36mbash: no job control in this shell\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:20,856 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:20,918 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:20,927 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:20,929 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[33mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[33mbash: no job control in this shell\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:20,844 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:20,904 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:20,913 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:20,915 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,860 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,920 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,929 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:20,931 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,841 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,903 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,912 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:20,914 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,812 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,873 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,882 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:20,884 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,723 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,796 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,654 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,728 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,800 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,838 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[34mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[34mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[34mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[34mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,626 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,698 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,770 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,803 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-8\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-8\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-8\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-8\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-8\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-8\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[32mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[32mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[32mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[32mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[32mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:24,703 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:24,776 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:24,850 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:24,887 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[35mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[35mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[35mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[35mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,696 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,770 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,844 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:24,876 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-3\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-3\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-3\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-3\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[32mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[32mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[32mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[32mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[32mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:24,675 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:24,748 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:24,823 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:24,861 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[36mTraining Env:\u001b[0m\n",
      "\u001b[36m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-4\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-4\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[36m}\u001b[0m\n",
      "\u001b[36mEnvironment variables:\u001b[0m\n",
      "\u001b[36mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[36mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[36mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[36mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[36mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[36mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[36mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[36mSM_CURRENT_HOST=algo-4\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[36mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[36mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[36mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[36mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[36mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[36mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[36mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[36mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[36mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[36mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[36mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[36mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-4\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[36mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[36mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[36mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[36mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[36mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[36mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[36mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[36mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[36mInvoking script with the following command:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:25,122 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:25,194 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:25,268 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:25,301 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[33mTraining Env:\u001b[0m\n",
      "\u001b[33m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-5\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-5\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[33m}\u001b[0m\n",
      "\u001b[33mEnvironment variables:\u001b[0m\n",
      "\u001b[33mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[33mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[33mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[33mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[33mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-5\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[33mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[33mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[33mSM_CURRENT_HOST=algo-5\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[33mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[33mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[33mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[33mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[33mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[33mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[33mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[33mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[33mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[33mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[33mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[33mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-5\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-5\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[33mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[33mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[33mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[33mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[33mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[33mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[33mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[33mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[33mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mInvoking script with the following command:\u001b[0m\n",
      "\u001b[33m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,870 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:24,907 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-6\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-6\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-6\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-6\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-6\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-6\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[34mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[34mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[34mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[34mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:25,159 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:25,233 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:25,307 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:25,344 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-7\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"conf\": \"gpt-neox/sai_vishwa_15B.yml\",\n",
      "        \"dp-degree\": 2,\n",
      "        \"dp-major\": \"True\",\n",
      "        \"entry-point\": \"gpt-neox/train_torchrun.py\",\n",
      "        \"optimize-for-pp\": \"False\",\n",
      "        \"pp-degree\": 4\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"topology-inference-mpi-2023-04-27-05-00-26-292\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mpi_entry_point\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-7\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mpi_entry_point.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mpi_entry_point.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-7\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-7\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mpi_entry_point\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-7\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{\"conf\":\"gpt-neox/sai_vishwa_15B.yml\",\"dp-degree\":2,\"dp-major\":\"True\",\"entry-point\":\"gpt-neox/train_torchrun.py\",\"optimize-for-pp\":\"False\",\"pp-degree\":4},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"topology-inference-mpi-2023-04-27-05-00-26-292\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/topology-inference-mpi-2023-04-27-05-00-26-292/source/sourcedir.tar.gz\",\"module_name\":\"mpi_entry_point\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-7\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mpi_entry_point.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--conf\",\"gpt-neox/sai_vishwa_15B.yml\",\"--dp-degree\",\"2\",\"--dp-major\",\"True\",\"--entry-point\",\"gpt-neox/train_torchrun.py\",\"--optimize-for-pp\",\"False\",\"--pp-degree\",\"4\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_CONF=gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[35mSM_HP_DP-DEGREE=2\u001b[0m\n",
      "\u001b[35mSM_HP_DP-MAJOR=True\u001b[0m\n",
      "\u001b[35mSM_HP_ENTRY-POINT=gpt-neox/train_torchrun.py\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZE-FOR-PP=False\u001b[0m\n",
      "\u001b[35mSM_HP_PP-DEGREE=4\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 mpi_entry_point.py --conf gpt-neox/sai_vishwa_15B.yml --dp-degree 2 --dp-major True --entry-point gpt-neox/train_torchrun.py --optimize-for-pp False --pp-degree 4\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:28,879 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:28,900 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:28,797 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:28,818 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mIn custom launcher\u001b[0m\n",
      "\u001b[32mworker\u001b[0m\n",
      "\u001b[32mInit worker runner\u001b[0m\n",
      "\u001b[32mworker init done\u001b[0m\n",
      "\u001b[32mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[32mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[32mConnection failed with exception: \n",
      " %s.              Can be ignored for worker when master completes and exits. [Errno None] Unable to connect to port 22 on 10.0.236.88\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:28,916 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:28,937 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mIn custom launcher\u001b[0m\n",
      "\u001b[35mworker\u001b[0m\n",
      "\u001b[35mInit worker runner\u001b[0m\n",
      "\u001b[35mworker init done\u001b[0m\n",
      "\u001b[35mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:28,879 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[32m2023-04-27 05:01:28,900 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mIn custom launcher\u001b[0m\n",
      "\u001b[32mworker\u001b[0m\n",
      "\u001b[32mInit worker runner\u001b[0m\n",
      "\u001b[32mworker init done\u001b[0m\n",
      "\u001b[32mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[32mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[32mConnection failed with exception: \n",
      " %s.              Can be ignored for worker when master completes and exits. [Errno None] Unable to connect to port 22 on 10.0.236.88\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:28,843 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[36m2023-04-27 05:01:28,864 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[36mIn custom launcher\u001b[0m\n",
      "\u001b[36mworker\u001b[0m\n",
      "\u001b[36mInit worker runner\u001b[0m\n",
      "\u001b[36mworker init done\u001b[0m\n",
      "\u001b[36mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[36mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[36mConnection failed with exception: \n",
      " %s.              Can be ignored for worker when master completes and exits. [Errno None] Unable to connect to port 22 on 10.0.236.88\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:29,271 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[33m2023-04-27 05:01:29,292 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[33mIn custom launcher\u001b[0m\n",
      "\u001b[33mworker\u001b[0m\n",
      "\u001b[33mInit worker runner\u001b[0m\n",
      "\u001b[33mworker init done\u001b[0m\n",
      "\u001b[33mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[33mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[33mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[33mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[33mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[33mstarting ssh daemon\u001b[0m\n",
      "\u001b[33mdone starting ssh daemon\u001b[0m\n",
      "\u001b[33mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:28,928 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-04-27 05:01:28,949 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mIn custom launcher\u001b[0m\n",
      "\u001b[34mworker\u001b[0m\n",
      "\u001b[34mInit worker runner\u001b[0m\n",
      "\u001b[34mworker init done\u001b[0m\n",
      "\u001b[34mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[34mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34mstarting ssh daemon\u001b[0m\n",
      "\u001b[34mdone starting ssh daemon\u001b[0m\n",
      "\u001b[34mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:29,354 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-04-27 05:01:29,375 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mIn custom launcher\u001b[0m\n",
      "\u001b[35mworker\u001b[0m\n",
      "\u001b[35mInit worker runner\u001b[0m\n",
      "\u001b[35mworker init done\u001b[0m\n",
      "\u001b[35mWorker waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[35mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[35mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35mstarting ssh daemon\u001b[0m\n",
      "\u001b[35mdone starting ssh daemon\u001b[0m\n",
      "\u001b[35mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34mIn custom launcher\u001b[0m\n",
      "\u001b[34mmaster\u001b[0m\n",
      "\u001b[34mInit master worker\u001b[0m\n",
      "\u001b[34mmaster init done\u001b[0m\n",
      "\u001b[34mmaster run called\u001b[0m\n",
      "\u001b[34mStarting MPI run as master node.\u001b[0m\n",
      "\u001b[34mCreating SSH daemon.\u001b[0m\n",
      "\u001b[34mstarting ssh daemon\u001b[0m\n",
      "\u001b[34mdone starting ssh daemon\u001b[0m\n",
      "\u001b[34mWaiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34mworkers: ['algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6', 'algo-7', 'algo-8']\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-2\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-2\u001b[0m\n",
      "\u001b[34mConnection failed with exception: \n",
      " %s.              Can be ignored for worker when master completes and exits. [Errno None] Unable to connect to port 22 on 10.0.229.58\u001b[0m\n",
      "\u001b[32mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[35mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[35mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35mstarting ssh daemon\u001b[0m\n",
      "\u001b[35mdone starting ssh daemon\u001b[0m\n",
      "\u001b[35mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[32mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[32mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[32mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[32mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[32mstarting ssh daemon\u001b[0m\n",
      "\u001b[32mdone starting ssh daemon\u001b[0m\n",
      "\u001b[32mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[36mTesting connection to host algo-1\u001b[0m\n",
      "\u001b[36mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[36mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[36mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[36mstarting ssh daemon\u001b[0m\n",
      "\u001b[36mdone starting ssh daemon\u001b[0m\n",
      "\u001b[36mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-2\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-2\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-2\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-3\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-3\u001b[0m\n",
      "\u001b[34mConnection failed with exception: \n",
      " %s.              Can be ignored for worker when master completes and exits. [Errno None] Unable to connect to port 22 on 10.0.226.42\u001b[0m\n",
      "\u001b[32mCan connect to host %s algo-1\u001b[0m\n",
      "\u001b[32mMPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[32mWriting environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[32mstarting ssh daemon\u001b[0m\n",
      "\u001b[32mdone starting ssh daemon\u001b[0m\n",
      "\u001b[32mWaiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-3\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-3\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-3\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-4\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-4\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-4\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-4\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-5\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-5\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-5\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-5\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-6\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-6\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-6\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-6\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-7\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-7\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-7\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-7\u001b[0m\n",
      "\u001b[34mmaster checking connection to algo-8\u001b[0m\n",
      "\u001b[34mTesting connection to host algo-8\u001b[0m\n",
      "\u001b[32mProcess[es]: %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[32mOrted process found %s\u001b[0m\n",
      "\u001b[32m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[32mWaiting for orted process %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[36mProcess[es]: %s\u001b[0m\n",
      "\u001b[36m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[36mOrted process found %s\u001b[0m\n",
      "\u001b[36m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[36mWaiting for orted process %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[33mProcess[es]: %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[33mOrted process found %s\u001b[0m\n",
      "\u001b[33m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[33mWaiting for orted process %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[34mProcess[es]: %s\u001b[0m\n",
      "\u001b[34m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[34mOrted process found %s\u001b[0m\n",
      "\u001b[34m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[34mWaiting for orted process %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[35mProcess[es]: %s\u001b[0m\n",
      "\u001b[35m[psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[35mOrted process found %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[35mWaiting for orted process %s [psutil.Process(pid=144, name='orted', status='running', started='05:01:32')]\u001b[0m\n",
      "\u001b[34mCan connect to host %s algo-8\u001b[0m\n",
      "\u001b[34mWorker %s available for communication algo-8\u001b[0m\n",
      "\u001b[34mEnv Hosts: %s Hosts: %s process_per_hosts: %s num_processes: %s\u001b[0m\n",
      "\u001b[34m['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6', 'algo-7', 'algo-8'] ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-5', 'algo-6', 'algo-7', 'algo-8'] 1 8\u001b[0m\n",
      "\u001b[34mcreated command mpirun --host algo-1,algo-2,algo-3,algo-4,algo-5,algo-6,algo-7,algo-8 -np 8 --allow-run-as-root bin/ring_latency_calculator /opt/ml/code\u001b[0m\n",
      "\u001b[34mWaiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-3,10.0.226.42' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-4,10.0.234.128' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-5,10.0.218.232' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-6,10.0.224.104' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-7,10.0.221.245' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.229.58' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-8,10.0.197.254' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34mInitializing MPI...\u001b[0m\n",
      "\u001b[34m0: 8: pid = 224\u001b[0m\n",
      "\u001b[34m6: 8: pid = 147\u001b[0m\n",
      "\u001b[34m1: 8: pid = 147\u001b[0m\n",
      "\u001b[34m3: 8: pid = 147\u001b[0m\n",
      "\u001b[34m4: 8: pid = 147\u001b[0m\n",
      "\u001b[34m2: 8: pid = 147\u001b[0m\n",
      "\u001b[34m5: 8: pid = 147\u001b[0m\n",
      "\u001b[34m7: 8: pid = 147\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34moutput_dir = /opt/ml/code/\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34mcurl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m1: Loading EFA devices...\u001b[0m\n",
      "\u001b[34m5: Loading EFA devices...\u001b[0m\n",
      "\u001b[34m3: Loading EFA devices...\u001b[0m\n",
      "\u001b[34m2: Loading EFA devices...\u001b[0m\n",
      "\u001b[34m7: Loading EFA devices...\u001b[0m\n",
      "\u001b[34m6: Loading EFA devices...\u001b[0m\n",
      "\u001b[34mInstance id mapping: \u001b[0m\n",
      "\u001b[34mNode 0 - algo-1\u001b[0m\n",
      "\u001b[34mNode 1 - algo-2\u001b[0m\n",
      "\u001b[34mNode 2 - algo-3\u001b[0m\n",
      "\u001b[34mNode 3 - algo-4\u001b[0m\n",
      "\u001b[34mNode 4 - algo-5\u001b[0m\n",
      "\u001b[34mNode 5 - algo-6\u001b[0m\n",
      "\u001b[34mNode 6 - algo-7\u001b[0m\n",
      "\u001b[34mNode 7 - algo-8\u001b[0m\n",
      "\u001b[34m0: Loading EFA devices...\u001b[0m\n",
      "\u001b[34m4: Loading EFA devices...\u001b[0m\n",
      "\u001b[32mProcess[es]: %s\u001b[0m\n",
      "\u001b[32m[psutil.Process(pid=144, name='orted', status='disk-sleep', started='05:01:32')]\u001b[0m\n",
      "\u001b[32mOrted process found %s [psutil.Process(pid=144, name='orted', status='disk-sleep', started='05:01:32')]\u001b[0m\n",
      "\u001b[32mWaiting for orted process %s\u001b[0m\n",
      "\u001b[32m[psutil.Process(pid=144, name='orted', status='disk-sleep', started='05:01:32')]\u001b[0m\n",
      "\u001b[35mProcess[es]: %s\u001b[0m\n",
      "\u001b[35m[psutil.Process(pid=144, name='orted', status='disk-sleep', started='05:01:31')]\u001b[0m\n",
      "\u001b[35mOrted process found %s\u001b[0m\n",
      "\u001b[35m[psutil.Process(pid=144, name='orted', status='disk-sleep', started='05:01:31')]\u001b[0m\n",
      "\u001b[35mWaiting for orted process %s [psutil.Process(pid=144, name='orted', status='disk-sleep', started='05:01:31')]\u001b[0m\n",
      "\u001b[35mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[35mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32') terminated with exit code None\u001b[0m\n",
      "\u001b[35mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32')] alive: []\u001b[0m\n",
      "\u001b[35mOrted process exited\u001b[0m\n",
      "\u001b[35mMPI process finished.\u001b[0m\n",
      "\u001b[35mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[35mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[35mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[35mLaunching job with command: torchrun --nnodes=8 --node_rank=5 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[34m0: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m4: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m5: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m7: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m6: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m1: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m2: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m3: All EFA devices have been loaded. Creating address handles now...\u001b[0m\n",
      "\u001b[34m2: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m6: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m1: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m5: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m0: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m7: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m4: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m3: Computing tolopogy information for the cluster...\u001b[0m\n",
      "\u001b[34m0: Avg: 34.69\u001b[0m\n",
      "\u001b[34m5: Avg: 35.6556\u001b[0m\n",
      "\u001b[34m1: Avg: 37.2589\u001b[0m\n",
      "\u001b[34m2: Avg: 37.7256\u001b[0m\n",
      "\u001b[34m7: Avg: 173.323\u001b[0m\n",
      "\u001b[34m4: Avg: 176.913\u001b[0m\n",
      "\u001b[34m6: Avg: 191.463\u001b[0m\n",
      "\u001b[34m3: Avg: 192.213\u001b[0m\n",
      "\u001b[34m0: Buckets: 0 0 0 0 1 0 0 1 \u001b[0m\n",
      "\u001b[34mCompute time: 230 ms\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mWriting topology to /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mDone waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[34mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[34mLaunching job with command: torchrun --nnodes=8 --node_rank=0 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[32mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[32mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32') terminated with exit code None\u001b[0m\n",
      "\u001b[32mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32')] alive: []\u001b[0m\n",
      "\u001b[32mOrted process exited\u001b[0m\n",
      "\u001b[32mMPI process finished.\u001b[0m\n",
      "\u001b[32mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[32mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[32mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[32mLaunching job with command: torchrun --nnodes=8 --node_rank=7 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[35mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[35mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:31') terminated with exit code None\u001b[0m\n",
      "\u001b[35mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:31')] alive: []\u001b[0m\n",
      "\u001b[35mOrted process exited\u001b[0m\n",
      "\u001b[35mMPI process finished.\u001b[0m\n",
      "\u001b[35mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[35mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[35mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[35mLaunching job with command: torchrun --nnodes=8 --node_rank=1 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[32mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[32mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32') terminated with exit code None\u001b[0m\n",
      "\u001b[32mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32')] alive: []\u001b[0m\n",
      "\u001b[32mOrted process exited\u001b[0m\n",
      "\u001b[32mMPI process finished.\u001b[0m\n",
      "\u001b[32mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[32mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[32mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[32mLaunching job with command: torchrun --nnodes=8 --node_rank=2 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[36mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[36mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32') terminated with exit code None\u001b[0m\n",
      "\u001b[36mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32')] alive: []\u001b[0m\n",
      "\u001b[36mOrted process exited\u001b[0m\n",
      "\u001b[36mMPI process finished.\u001b[0m\n",
      "\u001b[36mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[36mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[36mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[36mLaunching job with command: torchrun --nnodes=8 --node_rank=3 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[33mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[33mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32') terminated with exit code None\u001b[0m\n",
      "\u001b[33mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32')] alive: []\u001b[0m\n",
      "\u001b[33mOrted process exited\u001b[0m\n",
      "\u001b[33mMPI process finished.\u001b[0m\n",
      "\u001b[33mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[33mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[33mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[33mLaunching job with command: torchrun --nnodes=8 --node_rank=6 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[34mInvoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[34mprocess psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32') terminated with exit code None\u001b[0m\n",
      "\u001b[34mReporting status for ORTEd process. gone: [psutil.Process(pid=144, name='orted', status='terminated', started='05:01:32')] alive: []\u001b[0m\n",
      "\u001b[34mOrted process exited\u001b[0m\n",
      "\u001b[34mMPI process finished.\u001b[0m\n",
      "\u001b[34mReading topology mapping from file /opt/ml/code/cluster_topology.txt\u001b[0m\n",
      "\u001b[34mOutput from topology compute: {'spine1': ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7'], 'spine2': ['algo-5', 'algo-8']} count: 8\u001b[0m\n",
      "\u001b[34mranking is ['algo-1', 'algo-2', 'algo-3', 'algo-4', 'algo-6', 'algo-7', 'algo-5', 'algo-8']\u001b[0m\n",
      "\u001b[34mLaunching job with command: torchrun --nnodes=8 --node_rank=4 --nproc_per_node=8 --rdzv_endpoint=algo-1:29400 --rdzv_id=100 gpt-neox/train_torchrun.py --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[32mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[32mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[36mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[36m*****************************************\u001b[0m\n",
      "\u001b[36mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[36m*****************************************\u001b[0m\n",
      "\u001b[33mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[33m*****************************************\u001b[0m\n",
      "\u001b[33mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[33m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~~~~ local_rank ~~~  26\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~~~~ rank ~~~  2630\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~~~~ world_size ~~~  6464\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~~~~ local_rank ~~~  10\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~~~~ rank ~~~  ~~~ rank ~~~725 \u001b[0m\n",
      "\u001b[36m24\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~~~~ rank ~~~~~~ world_size ~~~   643164\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 29\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 27\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 28\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 49\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 52~~~ local_rank ~~~\n",
      " ~~~ world_size ~~~3 \u001b[0m\n",
      "\u001b[33m64\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~\u001b[0m\n",
      "\u001b[33m51\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 53\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 7~~~ local_rank ~~~\n",
      " ~~~ local_rank ~~~6~~~ rank ~~~\u001b[0m\n",
      "\u001b[33m55~~~ rank ~~~0\n",
      " \u001b[0m\n",
      "\u001b[33m54~~~ world_size ~~~\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~  6448~~~ world_size ~~~\n",
      " 64\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64~~~ local_rank ~~~\n",
      " 2\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 50\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 39\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 33\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 32\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 36\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 37\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 38\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~\u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 34\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 35\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 41\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 43\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~~~~ rank ~~~\u001b[0m\n",
      "\u001b[35m7~~~ local_rank ~~~45\n",
      " 4~~~ rank ~~~~~~ world_size ~~~\n",
      "  4764~~~ rank ~~~\n",
      " 44~~~ world_size ~~~\n",
      " ~~~ world_size ~~~64 \u001b[0m\n",
      "\u001b[35m64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 42\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 40\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 46\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 2\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~~~~ world_size ~~~  641\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 1~~~ local_rank ~~~\n",
      " ~~~ world_size ~~~6 \u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 6\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 0\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.configure_distributed_args() using world size: 64 and model-parallel size: 8\u001b[0m\n",
      "\u001b[34m> building GPT2BPETokenizer tokenizer ...\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 7\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 3\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 4\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 5\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m> padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)\u001b[0m\n",
      "\u001b[34m> initializing torch distributed ...\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:40,678] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m> initializing model parallel with size 8\u001b[0m\n",
      "\u001b[34mMPU DP: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34mMPU DP: [1, 9, 17, 25]\u001b[0m\n",
      "\u001b[34mMPU DP: [2, 10, 18, 26]\u001b[0m\n",
      "\u001b[34mMPU DP: [3, 11, 19, 27]\u001b[0m\n",
      "\u001b[34mMPU DP: [4, 12, 20, 28]\u001b[0m\n",
      "\u001b[34mMPU DP: [5, 13, 21, 29]\u001b[0m\n",
      "\u001b[34mMPU DP: [6, 14, 22, 30]\u001b[0m\n",
      "\u001b[34mMPU DP: [7, 15, 23, 31]\u001b[0m\n",
      "\u001b[34mMPU DP: [32, 40, 48, 56]\u001b[0m\n",
      "\u001b[34mMPU DP: [33, 41, 49, 57]\u001b[0m\n",
      "\u001b[34mMPU DP: [34, 42, 50, 58]\u001b[0m\n",
      "\u001b[34mMPU DP: [35, 43, 51, 59]\u001b[0m\n",
      "\u001b[34mMPU DP: [36, 44, 52, 60]\u001b[0m\n",
      "\u001b[34mMPU DP: [37, 45, 53, 61]\u001b[0m\n",
      "\u001b[34mMPU DP: [38, 46, 54, 62]\u001b[0m\n",
      "\u001b[34mMPU DP: [39, 47, 55, 63]\u001b[0m\n",
      "\u001b[34mMPU PP: [0, 32]\u001b[0m\n",
      "\u001b[34mMPU PP: [1, 33]\u001b[0m\n",
      "\u001b[34mMPU PP: [2, 34]\u001b[0m\n",
      "\u001b[34mMPU PP: [3, 35]\u001b[0m\n",
      "\u001b[34mMPU PP: [4, 36]\u001b[0m\n",
      "\u001b[34mMPU PP: [5, 37]\u001b[0m\n",
      "\u001b[34mMPU PP: [6, 38]\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 58\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~~~~ local_rank ~~~~~~ world_size ~~~\u001b[0m\n",
      "\u001b[32m0646\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ ~~~ rank ~~~56 \u001b[0m\n",
      "\u001b[32m62\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ ~~~ world_size ~~~64 \u001b[0m\n",
      "\u001b[32m64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 57\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 5~~~ rank ~~~\n",
      " 59\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 61\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ ~~~ world_size ~~~64 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 60\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 63\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 14\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 8\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 10\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 9\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 13\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 11\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 15\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 12\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 21\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 18\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 23\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 16\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 22\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 17\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 19\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 20\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~\u001b[0m\n",
      "\u001b[32m64\u001b[0m\n",
      "\u001b[36mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[36mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[36mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[33mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[33mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[33mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[35mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mMPU PP: [7, 39]\u001b[0m\n",
      "\u001b[34mMPU PP: [8, 40]\u001b[0m\n",
      "\u001b[34mMPU PP: [9, 41]\u001b[0m\n",
      "\u001b[34mMPU PP: [10, 42]\u001b[0m\n",
      "\u001b[34mMPU PP: [11, 43]\u001b[0m\n",
      "\u001b[34mMPU PP: [12, 44]\u001b[0m\n",
      "\u001b[34mMPU PP: [13, 45]\u001b[0m\n",
      "\u001b[34mMPU PP: [14, 46]\u001b[0m\n",
      "\u001b[34mMPU PP: [15, 47]\u001b[0m\n",
      "\u001b[34mMPU PP: [16, 48]\u001b[0m\n",
      "\u001b[34mMPU PP: [17, 49]\u001b[0m\n",
      "\u001b[34mMPU PP: [18, 50]\u001b[0m\n",
      "\u001b[34mMPU PP: [19, 51]\u001b[0m\n",
      "\u001b[34mMPU PP: [20, 52]\u001b[0m\n",
      "\u001b[34mMPU PP: [21, 53]\u001b[0m\n",
      "\u001b[34mMPU PP: [22, 54]\u001b[0m\n",
      "\u001b[34mMPU PP: [23, 55]\u001b[0m\n",
      "\u001b[34mMPU PP: [24, 56]\u001b[0m\n",
      "\u001b[34mMPU PP: [25, 57]\u001b[0m\n",
      "\u001b[34mMPU PP: [26, 58]\u001b[0m\n",
      "\u001b[34mMPU PP: [27, 59]\u001b[0m\n",
      "\u001b[34mMPU PP: [28, 60]\u001b[0m\n",
      "\u001b[34mMPU PP: [29, 61]\u001b[0m\n",
      "\u001b[34mMPU PP: [30, 62]\u001b[0m\n",
      "\u001b[34mMPU PP: [31, 63]\u001b[0m\n",
      "\u001b[34mMPU IO: [0, 8, 16, 24, 32, 40, 48, 56]\u001b[0m\n",
      "\u001b[34mMPU MP: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34mMPU MP: [8, 9, 10, 11, 12, 13, 14, 15]\u001b[0m\n",
      "\u001b[34mMPU MP: [16, 17, 18, 19, 20, 21, 22, 23]\u001b[0m\n",
      "\u001b[34mMPU MP: [24, 25, 26, 27, 28, 29, 30, 31]\u001b[0m\n",
      "\u001b[34mMPU MP: [32, 33, 34, 35, 36, 37, 38, 39]\u001b[0m\n",
      "\u001b[34mMPU MP: [40, 41, 42, 43, 44, 45, 46, 47]\u001b[0m\n",
      "\u001b[34mMPU MP: [48, 49, 50, 51, 52, 53, 54, 55]\u001b[0m\n",
      "\u001b[34mMPU MP: [56, 57, 58, 59, 60, 61, 62, 63]\u001b[0m\n",
      "\u001b[34m> setting random seeds to 1234 ...\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:41,378] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mbuilding GPT2 model ...\u001b[0m\n",
      "\u001b[34mSEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\u001b[0m\n",
      "\u001b[34mUsing topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=0, model=2): 2, ProcessCoord(pipe=0, data=0, model=3): 3, ProcessCoord(pipe=0, data=0, model=4): 4, ProcessCoord(pipe=0, data=0, model=5): 5, ProcessCoord(pipe=0, data=0, model=6): 6, ProcessCoord(pipe=0, data=0, model=7): 7, ProcessCoord(pipe=0, data=1, model=0): 8, ProcessCoord(pipe=0, data=1, model=1): 9, ProcessCoord(pipe=0, data=1, model=2): 10, ProcessCoord(pipe=0, data=1, model=3): 11, ProcessCoord(pipe=0, data=1, model=4): 12, ProcessCoord(pipe=0, data=1, model=5): 13, ProcessCoord(pipe=0, data=1, model=6): 14, ProcessCoord(pipe=0, data=1, model=7): 15, ProcessCoord(pipe=0, data=2, model=0): 16, ProcessCoord(pipe=0, data=2, model=1): 17, ProcessCoord(pipe=0, data=2, model=2): 18, ProcessCoord(pipe=0, data=2, model=3): 19, ProcessCoord(pipe=0, data=2, model=4): 20, ProcessCoord(pipe=0, data=2, model=5): 21, ProcessCoord(pipe=0, data=2, model=6): 22, ProcessCoord(pipe=0, data=2, model=7): 23, ProcessCoord(pipe=0, data=3, model=0): 24, ProcessCoord(pipe=0, data=3, model=1): 25, ProcessCoord(pipe=0, data=3, model=2): 26, ProcessCoord(pipe=0, data=3, model=3): 27, ProcessCoord(pipe=0, data=3, model=4): 28, ProcessCoord(pipe=0, data=3, model=5): 29, ProcessCoord(pipe=0, data=3, model=6): 30, ProcessCoord(pipe=0, data=3, model=7): 31, ProcessCoord(pipe=1, data=0, model=0): 32, ProcessCoord(pipe=1, data=0, model=1): 33, ProcessCoord(pipe=1, data=0, model=2): 34, ProcessCoord(pipe=1, data=0, model=3): 35, ProcessCoord(pipe=1, data=0, model=4): 36, ProcessCoord(pipe=1, data=0, model=5): 37, ProcessCoord(pipe=1, data=0, model=6): 38, ProcessCoord(pipe=1, data=0, model=7): 39, ProcessCoord(pipe=1, data=1, model=0): 40, ProcessCoord(pipe=1, data=1, model=1): 41, ProcessCoord(pipe=1, data=1, model=2): 42, ProcessCoord(pipe=1, data=1, model=3): 43, ProcessCoord(pipe=1, data=1, model=4): 44, ProcessCoord(pipe=1, data=1, model=5): 45, ProcessCoord(pipe=1, data=1, model=6): 46, ProcessCoord(pipe=1, data=1, model=7): 47, ProcessCoord(pipe=1, data=2, model=0): 48, ProcessCoord(pipe=1, data=2, model=1): 49, ProcessCoord(pipe=1, data=2, model=2): 50, ProcessCoord(pipe=1, data=2, model=3): 51, ProcessCoord(pipe=1, data=2, model=4): 52, ProcessCoord(pipe=1, data=2, model=5): 53, ProcessCoord(pipe=1, data=2, model=6): 54, ProcessCoord(pipe=1, data=2, model=7): 55, ProcessCoord(pipe=1, data=3, model=0): 56, ProcessCoord(pipe=1, data=3, model=1): 57, ProcessCoord(pipe=1, data=3, model=2): 58, ProcessCoord(pipe=1, data=3, model=3): 59, ProcessCoord(pipe=1, data=3, model=4): 60, ProcessCoord(pipe=1, data=3, model=5): 61, ProcessCoord(pipe=1, data=3, model=6): 62, ProcessCoord(pipe=1, data=3, model=7): 63}\u001b[0m\n",
      "\u001b[32mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[32mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[32mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[35mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[32mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[32mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[32mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:42,494] [INFO] [module.py:372:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp\u001b[0m\n",
      "\u001b[34mstage=0 layers=18\u001b[0m\n",
      "\u001b[34m0: EmbeddingPipe\n",
      "     1: _pre_transformer_block\u001b[0m\n",
      "\u001b[34m2: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m3: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m4: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m5: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m6: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m7: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m8: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m9: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m10: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m11: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m12: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m13: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m14: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m15: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m16: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m17: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34mstage=1 layers=19\u001b[0m\n",
      "\u001b[34m18: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m19: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m20: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m21: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m22: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m23: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m24: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m25: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m26: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m27: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m28: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m29: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m30: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m31: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m32: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m33: ParallelTransformerLayerPipe\n",
      "    34: _post_transformer_block\u001b[0m\n",
      "\u001b[34m35: NormPipe\u001b[0m\n",
      "\u001b[34m36: ParallelLinearPipe\n",
      "  loss: partial\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,138] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,152] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,167] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,229] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,229] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,229] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,230] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,259] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,167] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,173] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,199] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,248] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,249] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,252] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,255] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,260] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mConfiguring Optimizer type: Adam with params: {'lr': 0.00015, 'betas': [0.9, 0.95], 'eps': 1e-08}\u001b[0m\n",
      "\u001b[34m> learning rate decay style: cosine\u001b[0m\n",
      "\u001b[34mDeepSpeed is enabled.\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,131] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+67e89b7, git-hash=67e89b7, git-branch=v2.0-stability\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,170] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,172] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,183] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,184] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,206] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,236] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,239] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:44,243] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,179] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,199] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,217] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,239] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,240] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,242] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,244] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,245] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module utils...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[32mBuilding extension module utils...\u001b[0m\n",
      "\u001b[32mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,049] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,049] [INFO] [logging.py:77:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,049] [INFO] [logging.py:77:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [logging.py:77:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [stage_1_and_2.py:144:__init__] Reduce bucket size 1260000000\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [stage_1_and_2.py:145:__init__] Allgather bucket size 1260000000\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [stage_1_and_2.py:146:__init__] CPU Offload: False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:01:45,055] [INFO] [stage_1_and_2.py:147:__init__] Round robin gradient partitioning: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,149] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,205] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,226] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,232] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,237] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,243] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,243] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:01:44,247] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,173] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,183] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,189] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,215] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,228] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,237] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,239] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:01:44,253] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,075] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,177] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,214] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,224] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,232] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,233] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,233] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:01:44,262] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,161] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,198] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,224] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,243] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,246] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,252] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,291] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:01:44,305] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[32mBuilding extension module utils...\u001b[0m\n",
      "\u001b[32mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[36mBuilding extension module utils...\u001b[0m\n",
      "\u001b[36mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[33mBuilding extension module utils...\u001b[0m\n",
      "\u001b[33mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module utils...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[32m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[36m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[33m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.881832122802734 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.733751773834229 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.732104778289795 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.731560230255127 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.83226490020752 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.83213758468628 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.832690477371216 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.930602312088013 seconds\u001b[0m\n",
      "\u001b[32m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.509437561035156 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.43226671218872 secondsTime to load utils op: 14.432333946228027 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.43181586265564 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.52922248840332 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.42850375175476 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.430986166000366 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.432127952575684 seconds\u001b[0m\n",
      "\u001b[32m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.790788173675537 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.633514642715454 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.628398656845093 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.633107900619507 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.833495616912842 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.728228092193604 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.732410907745361 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.733124017715454 seconds\u001b[0m\n",
      "\u001b[36m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.760665655136108 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.62701940536499 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.626895189285278 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.63259243965149 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.627878427505493 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.630337715148926 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.627329587936401 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.82722544670105 seconds\u001b[0m\n",
      "\u001b[33m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.76776933670044 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.633161067962646 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.63282299041748 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.6312837600708 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.732266664505005 seconds\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.73244023323059 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.732903003692627 seconds\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.82903504371643 seconds\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.821768999099731 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732293128967285 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732454538345337 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.731886863708496 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.832659244537354 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.731238842010498 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732473134994507 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.731741189956665 seconds\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.655419111251831 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.527788400650024 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.527403354644775 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.532031297683716 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.527442932128906 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.526241302490234 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.531361103057861 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.732012271881104 seconds\u001b[0m\n",
      "\u001b[35m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.707452535629272 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.53158187866211 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.7304208278656 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.632746934890747 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.63263988494873 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.632729291915894 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.631683349609375 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.630273342132568 seconds\u001b[0m\n",
      "\u001b[36mRank: 29 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[33mRank: 51 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 36 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 34 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 39 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 35 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 37 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 45 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 44 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 46 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 4 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 6 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 1 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 0 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 3 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 2 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 5 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 7 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 59 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 60 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 61 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 57 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 63 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 58 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 56 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 62 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 11 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 15 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 14 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 12 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 10 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 8 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 13 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 9 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 23 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 19 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 18 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 20 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 16 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 17 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 21 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 22 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 31 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 25 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 26 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 30 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 27 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 24 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 28 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[33mRank: 52 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 53 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 48 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 54 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 55 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 49 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 50 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 33 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 32 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 38 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 47 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 43 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 42 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 41 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 40 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00041294097900390625 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00038552284240722656 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003795623779296875 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003590583801269531 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0005097389221191406 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003752708435058594 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003809928894042969 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003745555877685547 seconds\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003821849822998047 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003867149353027344 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0004584789276123047 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003368854522705078 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003914833068847656 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003390312194824219 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003325939178466797 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003879070281982422 seconds\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003764629364013672 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003421306610107422 seconds\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.00032782554626464844 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003848075866699219 seconds\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.000335693359375 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003428459167480469 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.00033855438232421875 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.000507354736328125 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00037217140197753906 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003437995910644531 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00033402442932128906 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00039768218994140625 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00033020973205566406 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004000663757324219 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005254745483398438 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00032138824462890625 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004525184631347656 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004017353057861328 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004935264587402344 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003898143768310547 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003857612609863281 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003764629364013672 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003762245178222656 seconds\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003762245178222656 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00047206878662109375 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00043392181396484375 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003578662872314453 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003390312194824219 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00044035911560058594 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00045180320739746094 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00039076805114746094 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,464] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,465] [INFO] [utils.py:830:see_memory_usage] MA 4.65 GB         Max_MA 5.42 GB         CA 4.67 GB         Max_CA 5 GB\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,465] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.97 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,580] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,581] [INFO] [utils.py:830:see_memory_usage] MA 7.75 GB         Max_MA 9.3 GB         CA 9.32 GB         Max_CA 9 GB\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,581] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.97 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,581] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,683] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,683] [INFO] [utils.py:830:see_memory_usage] MA 7.75 GB         Max_MA 7.75 GB         CA 9.32 GB         Max_CA 9 GB\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,684] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.97 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,685] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,685] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,685] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f10cb3ed8e0>\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,685] [INFO] [logging.py:77:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1022:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1022:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1022:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,686] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f10cb3edc70>\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   optimizer_name ............... adam\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   optimizer_params ............. {'lr': 0.00015, 'betas': [0.9, 0.95], 'eps': 1e-08}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,687] [INFO] [config.py:1022:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   steps_per_print .............. 1\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   train_batch_size ............. 4\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1260000000 allgather_partitions=True allgather_bucket_size=1260000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 1\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [config.py:1007:print_user_config]   json = {\n",
      "    \"train_batch_size\": 4, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.00015, \n",
      "            \"betas\": [0.9, 0.95], \n",
      "            \"eps\": 1e-08\n",
      "        }\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"fp16\": true, \n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 12, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 1, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 1.260000e+09, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 1.260000e+09, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"cpu_offload\": false\n",
      "    }, \n",
      "    \"steps_per_print\": 1\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00029277801513671875 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:07,688] [INFO] [engine.py:88:__init__] CONFIG: micro_batches=1 micro_batch_size=1\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00036215782165527344 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00034737586975097656 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00044536590576171875 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0004296302795410156 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.000331878662109375 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003845691680908203 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0004975795745849609 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003840923309326172 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00039315223693847656 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.000370025634765625 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003325939178466797 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003376007080078125 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00031447410583496094 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003349781036376953 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0005080699920654297 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003306865692138672 seconds\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,632] [INFO] [engine.py:144:__init__] RANK=34 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,632] [INFO] [engine.py:144:__init__] RANK=37 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=35 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=38 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=39 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=32 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=33 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=36 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=5 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=6 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=7 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=4 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=1 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=2 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=0 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:09,633] [INFO] [engine.py:144:__init__] RANK=3 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[32mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:02:10.342 algo-3:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[36mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:02:10.345 algo-4:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[33mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:02:10.344 algo-5:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[33m[2023-04-27 05:02:10.503 algo-5:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 7: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 5: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 3: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 1: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 2: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 0: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 6: 1663959040\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 4: 1663959040\u001b[0m\n",
      "\u001b[34mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:10.345 algo-6:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:10.534 algo-6:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:02:10.344 algo-7:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:02:10.505 algo-7:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 7: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 5: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 2: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 3: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 0: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 1: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 6: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 4: 1663942656\u001b[0m\n",
      "\u001b[34m> total params: 26,623,213,568\u001b[0m\n",
      "\u001b[34m> building train, validation, and test datasets ...\u001b[0m\n",
      "\u001b[34mreading sizes...\u001b[0m\n",
      "\u001b[34mreading pointers...\n",
      "    reading document index...\u001b[0m\n",
      "\u001b[34mcreating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\u001b[0m\n",
      "\u001b[34m> dataset split:\n",
      "    train:\u001b[0m\n",
      "\u001b[34mdocument indices in [0, 1) total of 1 documents\n",
      "    validation:\n",
      "     document indices in [1, 1) total of 0 documents\n",
      "    test:\n",
      "     document indices in [1, 1) total of 0 documents\u001b[0m\n",
      "\u001b[34m> loading doc-idx mapping from /opt/ml/input/data/train/enwik8/enwik8_text_document_train_indexmap_40ns_4096sl_1234s_doc_idx.npy\u001b[0m\n",
      "\u001b[34m> loading sample-idx mapping from /opt/ml/input/data/train/enwik8/enwik8_text_document_train_indexmap_40ns_4096sl_1234s_sample_idx.npy\u001b[0m\n",
      "\u001b[34m> loading shuffle-idx mapping from /opt/ml/input/data/train/enwik8/enwik8_text_document_train_indexmap_40ns_4096sl_1234s_shuffle_idx.npy\u001b[0m\n",
      "\u001b[34mloaded indexed file in 0.003 seconds\n",
      "    total number of samples: 7084\u001b[0m\n",
      "\u001b[34mtotal number of epochs: 1\u001b[0m\n",
      "\u001b[34mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[34msetting training data start iteration to 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:10.337 algo-1:305 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:10.365 algo-1:305 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34mdone with setups ...\u001b[0m\n",
      "\u001b[34mtime (ms) | model and optimizer: 28304.88 | train/valid/test data iterators: 279.38\u001b[0m\n",
      "\u001b[34mtraining ...\u001b[0m\n",
      "\u001b[32mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:02:10.343 algo-8:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:02:10.502 algo-8:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:02:10.346 algo-2:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-04-27 05:02:10.546 algo-2:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32m[2023-04-27 05:02:10.505 algo-3:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[36m[2023-04-27 05:02:10.543 algo-4:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:12,258] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:12,258] [INFO] [checkpointing.py:554:forward] ----Partition Activations True, CPU CHECKPOINTING False\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:12,258] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 32 total layers\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:12,258] [INFO] [checkpointing.py:560:forward] ----Synchronization True\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:12,258] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:23,169] [INFO] [logging.py:77:log_dist] [Rank 0] step=1, skipped=0, lr=[4.166666666666666e-08, 4.166666666666666e-08], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 1 loss: 12.5466 iter time (s): 13.743 samples/sec: 0.291\u001b[0m\n",
      "\u001b[34msamples/sec: 0.291 | iteration        1/      10 | elapsed time per iteration (ms): 13747.1 | learning rate: 4.167E-08 | approx flops per GPU: 4.2TFLOPS | lm_loss: 1.254662E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mafter 1 iterations memory (MB) | allocated: 7966.37060546875 | max allocated: 13480.0888671875 | reserved: 18964.0 | max reserved: 18964.0\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:26,027] [INFO] [logging.py:77:log_dist] [Rank 0] step=2, skipped=0, lr=[8.333333333333333e-08, 8.333333333333333e-08], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 2 loss: 12.5746 iter time (s): 2.539 samples/sec: 1.575\u001b[0m\n",
      "\u001b[34msamples/sec: 1.574 | iteration        2/      10 | elapsed time per iteration (ms): 2541.1 | learning rate: 8.333E-08 | approx flops per GPU: 22.6TFLOPS | lm_loss: 1.257463E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:28,545] [INFO] [logging.py:77:log_dist] [Rank 0] step=3, skipped=0, lr=[1.25e-07, 1.25e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 3 loss: 12.4993 iter time (s): 2.526 samples/sec: 1.584\u001b[0m\n",
      "\u001b[34msamples/sec: 1.583 | iteration        3/      10 | elapsed time per iteration (ms): 2526.9 | learning rate: 1.250E-07 | approx flops per GPU: 22.7TFLOPS | lm_loss: 1.249934E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:31,074] [INFO] [logging.py:77:log_dist] [Rank 0] step=4, skipped=0, lr=[1.6666666666666665e-07, 1.6666666666666665e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 4 loss: 12.4597 iter time (s): 2.523 samples/sec: 1.585\u001b[0m\n",
      "\u001b[34msamples/sec: 1.585 | iteration        4/      10 | elapsed time per iteration (ms): 2524.4 | learning rate: 1.667E-07 | approx flops per GPU: 22.7TFLOPS | lm_loss: 1.245971E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:33,595] [INFO] [logging.py:77:log_dist] [Rank 0] step=5, skipped=0, lr=[2.083333333333333e-07, 2.083333333333333e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 5 loss: 12.5176 iter time (s): 2.529 samples/sec: 1.582\u001b[0m\n",
      "\u001b[34msamples/sec: 1.581 | iteration        5/      10 | elapsed time per iteration (ms): 2529.9 | learning rate: 2.083E-07 | approx flops per GPU: 22.7TFLOPS | lm_loss: 1.251763E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:36,120] [INFO] [logging.py:77:log_dist] [Rank 0] step=6, skipped=0, lr=[2.5e-07, 2.5e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 6 loss: 11.9254 iter time (s): 2.515 samples/sec: 1.590\u001b[0m\n",
      "\u001b[34msamples/sec: 1.589 | iteration        6/      10 | elapsed time per iteration (ms): 2516.5 | learning rate: 2.500E-07 | approx flops per GPU: 22.8TFLOPS | lm_loss: 1.192541E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:38,636] [INFO] [logging.py:77:log_dist] [Rank 0] step=7, skipped=0, lr=[2.9166666666666664e-07, 2.9166666666666664e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 7 loss: 11.0721 iter time (s): 2.531 samples/sec: 1.580\u001b[0m\n",
      "\u001b[34msamples/sec: 1.580 | iteration        7/      10 | elapsed time per iteration (ms): 2532.4 | learning rate: 2.917E-07 | approx flops per GPU: 22.7TFLOPS | lm_loss: 1.107206E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:41,166] [INFO] [logging.py:77:log_dist] [Rank 0] step=8, skipped=0, lr=[3.333333333333333e-07, 3.333333333333333e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 8 loss: 10.7025 iter time (s): 2.519 samples/sec: 1.588\u001b[0m\n",
      "\u001b[34msamples/sec: 1.587 | iteration        8/      10 | elapsed time per iteration (ms): 2520.0 | learning rate: 3.333E-07 | approx flops per GPU: 22.8TFLOPS | lm_loss: 1.070254E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:43,685] [INFO] [logging.py:77:log_dist] [Rank 0] step=9, skipped=0, lr=[3.7499999999999996e-07, 3.7499999999999996e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 9 loss: 11.9615 iter time (s): 2.515 samples/sec: 1.591\u001b[0m\n",
      "\u001b[34msamples/sec: 1.590 | iteration        9/      10 | elapsed time per iteration (ms): 2516.1 | learning rate: 3.750E-07 | approx flops per GPU: 22.8TFLOPS | lm_loss: 1.196148E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 05:02:46,212] [INFO] [logging.py:77:log_dist] [Rank 0] step=10, skipped=0, lr=[4.166666666666666e-07, 4.166666666666666e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 10 loss: 12.0738 iter time (s): 2.534 samples/sec: 1.579\u001b[0m\n",
      "\u001b[34msamples/sec: 1.578 | iteration       10/      10 | elapsed time per iteration (ms): 2535.1 | learning rate: 4.167E-07 | approx flops per GPU: 22.6TFLOPS | lm_loss: 1.207377E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[32mJob finished!\u001b[0m\n",
      "\u001b[36mJob finished!\u001b[0m\n",
      "\u001b[33mJob finished!\u001b[0m\n",
      "\u001b[34mJob finished!\u001b[0m\n",
      "\u001b[35mJob finished!\u001b[0m\n",
      "\u001b[34mJob finished!\u001b[0m\n",
      "\u001b[32mJob finished!\u001b[0m\n",
      "\u001b[35mJob finished!\u001b[0m\n",
      "\n",
      "2023-04-27 05:03:28 Stopping - Stopping the training job\n",
      "2023-04-27 05:03:28 Uploading - Uploading generated training model\n",
      "2023-04-27 05:03:28 Stopped - Resource retained for reuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 1192\n",
      "Billable seconds: 1192\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "s3_train_bucket = \"s3://viskaria/enwik8\"\n",
    "\n",
    "train = sagemaker.inputs.TrainingInput(\n",
    "            s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "        )\n",
    "data_channels = {\"train\": train}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"topology-inference-mpi\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"mpi_entry_point.py\",\n",
    "    role=role,\n",
    "    image_uri=\"570106654206.dkr.ecr.us-east-1.amazonaws.com/megatron-deepspeed:pt1.13.1-deeperspeed-0.8.3-v1\",\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=8,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    debugger_hook_config=False,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    hyperparameters = {'pp-degree': 4, 'dp-degree': 2,\n",
    "                       'optimize-for-pp': 'False', 'dp-major': 'True', \n",
    "                       'entry-point': 'gpt-neox/train_torchrun.py', \n",
    "                       'conf': 'gpt-neox/sai_vishwa_15B.yml', \n",
    "                       }\n",
    ")\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Megatron+DS with gpt-neox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure FSx Input for your SageMaker Training job\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# FSx file system ID with your training dataset. Example: 'fs-0bYYYYYY'\n",
    "file_system_id = \"fs-058ddb4f9d002316a\"\n",
    "# FSx path with your training data # Example: '/fsx_mount_name/imagenet'\n",
    "file_system_directory_path = \"/evoynbev/enwik8\"\n",
    "file_system_access_mode = \"rw\"\n",
    "file_system_type = \"FSxLustre\"\n",
    "train_fs = FileSystemInput(\n",
    "    file_system_id=file_system_id,\n",
    "    file_system_type=file_system_type,\n",
    "    directory_path=file_system_directory_path,\n",
    "    file_system_access_mode=file_system_access_mode,\n",
    ")\n",
    "# Specify the training data channel using the FSx filesystem. This will be provided to the SageMaker training job later\n",
    "data_channels = {\"train\": train_fs}\n",
    "\n",
    "# The following variables will be used later in the notebook\n",
    "subnets = [\"subnet-8caa97c1\"]  # Should be the subnet used for FSx. Example: subnet-0f9XXXX\n",
    "security_group_ids = [\"sg-fa5abbfb\"]  # Should be the security group used for FSx. sg-03ZZZZZZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_train_bucket = \"s3://viskaria/enwik8\"\n",
    "\n",
    "train = sagemaker.inputs.TrainingInput(\n",
    "            s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "        )\n",
    "data_channels = {\"train\": train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: mds-gpt-neox-2023-04-27-04-10-41-916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 04:11:09 Starting - Starting the training job...\n",
      "2023-04-27 04:11:19 Downloading - Downloading input data\n",
      "2023-04-27 04:11:19 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,833 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,896 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,905 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,907 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,795 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,857 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,866 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,868 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,811 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,872 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,881 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,883 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[36mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[36mbash: no job control in this shell\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:31,818 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:31,879 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:31,888 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:31,890 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[33mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[33mbash: no job control in this shell\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:31,751 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:31,813 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:31,822 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:31,824 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,773 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,834 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,843 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:31,845 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,784 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,846 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,855 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:31,857 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[32mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[32mbash: no job control in this shell\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,764 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,825 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,834 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:31,836 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,587 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,608 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,681 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,753 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,698 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,771 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,755 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,829 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,903 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,935 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,587 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,659 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,733 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,771 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-8\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-8\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-8\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-8\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-8\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-8\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,633 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,706 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,780 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:35,812 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[32mTraining Env:\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-3\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-3\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32mEnvironment variables:\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[32mSM_HPS={}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-3\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[32mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[32mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[32mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[32mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[32mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-3\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-3\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\u001b[0m\n",
      "\u001b[32m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:35,634 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:35,709 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:35,782 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:35,814 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[36mTraining Env:\u001b[0m\n",
      "\u001b[36m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-4\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-4\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[36m}\u001b[0m\n",
      "\u001b[36mEnvironment variables:\u001b[0m\n",
      "\u001b[36mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[36mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[36mSM_HPS={}\u001b[0m\n",
      "\u001b[36mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[36mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[36mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[36mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[36mSM_CURRENT_HOST=algo-4\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[36mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[36mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[36mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[36mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[36mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[36mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[36mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[36mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[36mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[36mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[36mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[36mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[36mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[36mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[36mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-4\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-4\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[36mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[36mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[36mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[36mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[36mInvoking script with the following command:\u001b[0m\n",
      "\u001b[36m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:35,839 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:35,911 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:35,984 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:36,021 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[33mTraining Env:\u001b[0m\n",
      "\u001b[33m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-5\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-5\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[33m}\u001b[0m\n",
      "\u001b[33mEnvironment variables:\u001b[0m\n",
      "\u001b[33mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[33mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[33mSM_HPS={}\u001b[0m\n",
      "\u001b[33mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[33mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-5\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[33mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[33mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[33mSM_CURRENT_HOST=algo-5\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[33mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[33mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[33mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[33mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[33mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[33mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[33mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[33mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[33mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[33mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[33mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[33mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[33mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[33mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[33mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-5\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-5\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[33mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[33mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[33mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[33mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mInvoking script with the following command:\u001b[0m\n",
      "\u001b[33m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,661 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,734 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,772 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-6\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-6\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-6\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-6\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-6\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-6\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:35,786 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-7\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-7\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-7\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-7\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-7\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-7\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,843 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:35,875 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-5\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\",\n",
      "        \"algo-2\",\n",
      "        \"algo-6\",\n",
      "        \"algo-4\",\n",
      "        \"algo-3\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\",\n",
      "        \"algo-3\",\n",
      "        \"algo-4\",\n",
      "        \"algo-5\",\n",
      "        \"algo-6\",\n",
      "        \"algo-7\",\n",
      "        \"algo-8\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-5\",\n",
      "                \"algo-7\",\n",
      "                \"algo-8\",\n",
      "                \"algo-2\",\n",
      "                \"algo-6\",\n",
      "                \"algo-4\",\n",
      "                \"algo-3\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"mds-gpt-neox-2023-04-27-04-10-41-916\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gpt-neox/run_torchrun\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\",\n",
      "            \"algo-3\",\n",
      "            \"algo-4\",\n",
      "            \"algo-5\",\n",
      "            \"algo-6\",\n",
      "            \"algo-7\",\n",
      "            \"algo-8\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-5\",\n",
      "                    \"algo-7\",\n",
      "                    \"algo-8\",\n",
      "                    \"algo-2\",\n",
      "                    \"algo-6\",\n",
      "                    \"algo-4\",\n",
      "                    \"algo-3\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gpt-neox/run_torchrun.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=gpt-neox/run_torchrun\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mds-gpt-neox-2023-04-27-04-10-41-916\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-570106654206/mds-gpt-neox-2023-04-27-04-10-41-916/source/sourcedir.tar.gz\",\"module_name\":\"gpt-neox/run_torchrun\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\",\"algo-3\",\"algo-4\",\"algo-5\",\"algo-6\",\"algo-7\",\"algo-8\"],\"instance_groups\":[{\"hosts\":[\"algo-5\",\"algo-7\",\"algo-8\",\"algo-2\",\"algo-6\",\"algo-4\",\"algo-3\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gpt-neox/run_torchrun.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 gpt-neox/run_torchrun.py\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:39,762 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:39,783 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mtorchrun --nnodes=8 --node_rank=7 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:39,874 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:11:39,895 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[32mtorchrun --nnodes=8 --node_rank=2 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:39,839 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[36m2023-04-27 04:11:39,860 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[36mtorchrun --nnodes=8 --node_rank=3 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:40,037 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[33m2023-04-27 04:11:40,058 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[33mtorchrun --nnodes=8 --node_rank=4 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:39,780 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:39,801 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes=8 --node_rank=5 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:39,826 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:39,848 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes=8 --node_rank=6 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:39,870 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:11:39,891 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes=8 --node_rank=0 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:40,023 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:11:40,044 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes=8 --node_rank=1 --nproc_per_node=8         --rdzv_endpoint='algo-1:50000' --rdzv_id=100  gpt-neox/train_torchrun.py         --conf gpt-neox/sai_vishwa_15B.yml\u001b[0m\n",
      "\u001b[32mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[32mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[32m*****************************************\u001b[0m\n",
      "\u001b[36mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[36m*****************************************\u001b[0m\n",
      "\u001b[36mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[36m*****************************************\u001b[0m\n",
      "\u001b[33mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[33m*****************************************\u001b[0m\n",
      "\u001b[33mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[33m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[34mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[35mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[36mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 37\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~~~~ local_rank ~~~  32\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ ~~~ rank ~~~35\u001b[0m\n",
      "\u001b[33m34~~~ world_size ~~~\u001b[0m\n",
      "\u001b[33m64\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~~~~ local_rank ~~~  10\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~~~~ rank ~~~  3332\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ ~~~ world_size ~~~64\u001b[0m\n",
      "\u001b[33m64\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 36\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 45\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~~~~ world_size ~~~  640\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 40\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 42\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 43\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 46\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 47\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 44\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 41\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 54\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 49\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 52\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64~~~ local_rank ~~~\u001b[0m\n",
      "\u001b[35m0\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 48\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 50\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 53\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 55\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 51\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 7\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 5\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ ~~~ local_rank ~~~ 34\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ ~~~ rank ~~~3\u001b[0m\n",
      "\u001b[34m4\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ ~~~ world_size ~~~64\u001b[0m\n",
      "\u001b[34m64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 1\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 2\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 6\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[34m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[34m~~~ rank ~~~ 0\u001b[0m\n",
      "\u001b[34m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[34mNeoXArgs.configure_distributed_args() using world size: 64 and model-parallel size: 8\u001b[0m\n",
      "\u001b[34m> building GPT2BPETokenizer tokenizer ...\u001b[0m\n",
      "\u001b[34m> padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)\u001b[0m\n",
      "\u001b[34m> initializing torch distributed ...\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:46,720] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m> initializing model parallel with size 8\u001b[0m\n",
      "\u001b[34mMPU DP: [0, 8, 16, 24]\u001b[0m\n",
      "\u001b[34mMPU DP: [1, 9, 17, 25]\u001b[0m\n",
      "\u001b[34mMPU DP: [2, 10, 18, 26]\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 8\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 12\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 11\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 9\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 15\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 14\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 10\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[35m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[35m~~~ rank ~~~ 13\u001b[0m\n",
      "\u001b[35m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~~~~ rank ~~~  021\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ ~~~ rank ~~~64\u001b[0m\n",
      "\u001b[32m16\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 20\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 17\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 23\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 18\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 19\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 22\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 25\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']NeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 26\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 30\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 31\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 29\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 27\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 28\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[36mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[36m~~~ local_rank ~~~ 0\u001b[0m\n",
      "\u001b[36m~~~ rank ~~~ 24\u001b[0m\n",
      "\u001b[36m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 6\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 38\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[33m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[33m~~~ rank ~~~ 39\u001b[0m\n",
      "\u001b[33m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[33mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[33mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[33mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[35mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mMPU DP: [3, 11, 19, 27]\u001b[0m\n",
      "\u001b[34mMPU DP: [4, 12, 20, 28]\u001b[0m\n",
      "\u001b[34mMPU DP: [5, 13, 21, 29]\u001b[0m\n",
      "\u001b[34mMPU DP: [6, 14, 22, 30]\u001b[0m\n",
      "\u001b[34mMPU DP: [7, 15, 23, 31]\u001b[0m\n",
      "\u001b[34mMPU DP: [32, 40, 48, 56]\u001b[0m\n",
      "\u001b[34mMPU DP: [33, 41, 49, 57]\u001b[0m\n",
      "\u001b[34mMPU DP: [34, 42, 50, 58]\u001b[0m\n",
      "\u001b[34mMPU DP: [35, 43, 51, 59]\u001b[0m\n",
      "\u001b[34mMPU DP: [36, 44, 52, 60]\u001b[0m\n",
      "\u001b[34mMPU DP: [37, 45, 53, 61]\u001b[0m\n",
      "\u001b[34mMPU DP: [38, 46, 54, 62]\u001b[0m\n",
      "\u001b[34mMPU DP: [39, 47, 55, 63]\u001b[0m\n",
      "\u001b[34mMPU PP: [0, 32]\u001b[0m\n",
      "\u001b[34mMPU PP: [1, 33]\u001b[0m\n",
      "\u001b[34mMPU PP: [2, 34]\u001b[0m\n",
      "\u001b[34mMPU PP: [3, 35]\u001b[0m\n",
      "\u001b[34mMPU PP: [4, 36]\u001b[0m\n",
      "\u001b[34mMPU PP: [5, 37]\u001b[0m\n",
      "\u001b[34mMPU PP: [6, 38]\u001b[0m\n",
      "\u001b[34mMPU PP: [7, 39]\u001b[0m\n",
      "\u001b[34mMPU PP: [8, 40]\u001b[0m\n",
      "\u001b[34mMPU PP: [9, 41]\u001b[0m\n",
      "\u001b[34mMPU PP: [10, 42]\u001b[0m\n",
      "\u001b[34mMPU PP: [11, 43]\u001b[0m\n",
      "\u001b[34mMPU PP: [12, 44]\u001b[0m\n",
      "\u001b[34mMPU PP: [13, 45]\u001b[0m\n",
      "\u001b[34mMPU PP: [14, 46]\u001b[0m\n",
      "\u001b[34mMPU PP: [15, 47]\u001b[0m\n",
      "\u001b[34mMPU PP: [16, 48]\u001b[0m\n",
      "\u001b[34mMPU PP: [17, 49]\u001b[0m\n",
      "\u001b[34mMPU PP: [18, 50]\u001b[0m\n",
      "\u001b[34mMPU PP: [19, 51]\u001b[0m\n",
      "\u001b[34mMPU PP: [20, 52]\u001b[0m\n",
      "\u001b[34mMPU PP: [21, 53]\u001b[0m\n",
      "\u001b[34mMPU PP: [22, 54]\u001b[0m\n",
      "\u001b[34mMPU PP: [23, 55]\u001b[0m\n",
      "\u001b[34mMPU PP: [24, 56]\u001b[0m\n",
      "\u001b[34mMPU PP: [25, 57]\u001b[0m\n",
      "\u001b[34mMPU PP: [26, 58]\u001b[0m\n",
      "\u001b[34mMPU PP: [27, 59]\u001b[0m\n",
      "\u001b[34mMPU PP: [28, 60]\u001b[0m\n",
      "\u001b[34mMPU PP: [29, 61]\u001b[0m\n",
      "\u001b[34mMPU PP: [30, 62]\u001b[0m\n",
      "\u001b[34mMPU PP: [31, 63]\u001b[0m\n",
      "\u001b[34mMPU IO: [0, 8, 16, 24, 32, 40, 48, 56]\u001b[0m\n",
      "\u001b[34mMPU MP: [0, 1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[34mMPU MP: [8, 9, 10, 11, 12, 13, 14, 15]\u001b[0m\n",
      "\u001b[34mMPU MP: [16, 17, 18, 19, 20, 21, 22, 23]\u001b[0m\n",
      "\u001b[34mMPU MP: [24, 25, 26, 27, 28, 29, 30, 31]\u001b[0m\n",
      "\u001b[34mMPU MP: [32, 33, 34, 35, 36, 37, 38, 39]\u001b[0m\n",
      "\u001b[34mMPU MP: [40, 41, 42, 43, 44, 45, 46, 47]\u001b[0m\n",
      "\u001b[34mMPU MP: [48, 49, 50, 51, 52, 53, 54, 55]\u001b[0m\n",
      "\u001b[34mMPU MP: [56, 57, 58, 59, 60, 61, 62, 63]\u001b[0m\n",
      "\u001b[34m> setting random seeds to 1234 ...\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:47,398] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\u001b[0m\n",
      "\u001b[34mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[34mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[35mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[35mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[32mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[32mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[32mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[36mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[36mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[36mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[34mbuilding GPT2 model ...\u001b[0m\n",
      "\u001b[34mSEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\u001b[0m\n",
      "\u001b[34mUsing topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=0, model=2): 2, ProcessCoord(pipe=0, data=0, model=3): 3, ProcessCoord(pipe=0, data=0, model=4): 4, ProcessCoord(pipe=0, data=0, model=5): 5, ProcessCoord(pipe=0, data=0, model=6): 6, ProcessCoord(pipe=0, data=0, model=7): 7, ProcessCoord(pipe=0, data=1, model=0): 8, ProcessCoord(pipe=0, data=1, model=1): 9, ProcessCoord(pipe=0, data=1, model=2): 10, ProcessCoord(pipe=0, data=1, model=3): 11, ProcessCoord(pipe=0, data=1, model=4): 12, ProcessCoord(pipe=0, data=1, model=5): 13, ProcessCoord(pipe=0, data=1, model=6): 14, ProcessCoord(pipe=0, data=1, model=7): 15, ProcessCoord(pipe=0, data=2, model=0): 16, ProcessCoord(pipe=0, data=2, model=1): 17, ProcessCoord(pipe=0, data=2, model=2): 18, ProcessCoord(pipe=0, data=2, model=3): 19, ProcessCoord(pipe=0, data=2, model=4): 20, ProcessCoord(pipe=0, data=2, model=5): 21, ProcessCoord(pipe=0, data=2, model=6): 22, ProcessCoord(pipe=0, data=2, model=7): 23, ProcessCoord(pipe=0, data=3, model=0): 24, ProcessCoord(pipe=0, data=3, model=1): 25, ProcessCoord(pipe=0, data=3, model=2): 26, ProcessCoord(pipe=0, data=3, model=3): 27, ProcessCoord(pipe=0, data=3, model=4): 28, ProcessCoord(pipe=0, data=3, model=5): 29, ProcessCoord(pipe=0, data=3, model=6): 30, ProcessCoord(pipe=0, data=3, model=7): 31, ProcessCoord(pipe=1, data=0, model=0): 32, ProcessCoord(pipe=1, data=0, model=1): 33, ProcessCoord(pipe=1, data=0, model=2): 34, ProcessCoord(pipe=1, data=0, model=3): 35, ProcessCoord(pipe=1, data=0, model=4): 36, ProcessCoord(pipe=1, data=0, model=5): 37, ProcessCoord(pipe=1, data=0, model=6): 38, ProcessCoord(pipe=1, data=0, model=7): 39, ProcessCoord(pipe=1, data=1, model=0): 40, ProcessCoord(pipe=1, data=1, model=1): 41, ProcessCoord(pipe=1, data=1, model=2): 42, ProcessCoord(pipe=1, data=1, model=3): 43, ProcessCoord(pipe=1, data=1, model=4): 44, ProcessCoord(pipe=1, data=1, model=5): 45, ProcessCoord(pipe=1, data=1, model=6): 46, ProcessCoord(pipe=1, data=1, model=7): 47, ProcessCoord(pipe=1, data=2, model=0): 48, ProcessCoord(pipe=1, data=2, model=1): 49, ProcessCoord(pipe=1, data=2, model=2): 50, ProcessCoord(pipe=1, data=2, model=3): 51, ProcessCoord(pipe=1, data=2, model=4): 52, ProcessCoord(pipe=1, data=2, model=5): 53, ProcessCoord(pipe=1, data=2, model=6): 54, ProcessCoord(pipe=1, data=2, model=7): 55, ProcessCoord(pipe=1, data=3, model=0): 56, ProcessCoord(pipe=1, data=3, model=1): 57, ProcessCoord(pipe=1, data=3, model=2): 58, ProcessCoord(pipe=1, data=3, model=3): 59, ProcessCoord(pipe=1, data=3, model=4): 60, ProcessCoord(pipe=1, data=3, model=5): 61, ProcessCoord(pipe=1, data=3, model=6): 62, ProcessCoord(pipe=1, data=3, model=7): 63}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:48,500] [INFO] [module.py:372:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp\u001b[0m\n",
      "\u001b[34mstage=0 layers=18\u001b[0m\n",
      "\u001b[34m0: EmbeddingPipe\u001b[0m\n",
      "\u001b[34m1: _pre_transformer_block\u001b[0m\n",
      "\u001b[34m2: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m3: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m4: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m5: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m6: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m7: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m8: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m9: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m10: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m11: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m12: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m13: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m14: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m15: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m16: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m17: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34mstage=1 layers=19\u001b[0m\n",
      "\u001b[34m18: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m19: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m20: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m21: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m22: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m23: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m24: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m25: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m26: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m27: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m28: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m29: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m30: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m31: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m32: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m33: ParallelTransformerLayerPipe\u001b[0m\n",
      "\u001b[34m34: _post_transformer_block\u001b[0m\n",
      "\u001b[34m35: NormPipe\u001b[0m\n",
      "\u001b[34m36: ParallelLinearPipe\u001b[0m\n",
      "\u001b[34mloss: partial\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,195] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,197] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,219] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,230] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,125] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,156] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,162] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,188] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,218] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,227] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,234] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[36mHost: 10.0.234.128\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:11:50,244] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mfatal: not a git repository (or any of the parent directories): .git\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 5\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 61\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 3\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 59\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 4\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 60\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 1\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 57\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 7\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 63\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32mNeoXArgs.from_ymls() ['gpt-neox/sai_vishwa_15B.yml']\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~~~~ local_rank ~~~  60\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ ~~~ rank ~~~62\u001b[0m\n",
      "\u001b[32m56\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ ~~~ world_size ~~~64\u001b[0m\n",
      "\u001b[32m64\u001b[0m\n",
      "\u001b[32m~~~ local_rank ~~~ 2\u001b[0m\n",
      "\u001b[32m~~~ rank ~~~ 58\u001b[0m\n",
      "\u001b[32m~~~ world_size ~~~ 64\u001b[0m\n",
      "\u001b[32mmake: Entering directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[32mmake: Nothing to be done for 'default'.\u001b[0m\n",
      "\u001b[32mmake: Leaving directory '/opt/ml/code/gpt-neox/megatron/data'\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,170] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,174] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,222] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,259] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,262] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,265] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,271] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mHost: 10.0.218.232\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:11:50,293] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[33mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,151] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,233] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,248] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,249] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,250] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,250] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,259] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.224.104\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,292] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,174] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,225] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,235] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,242] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,250] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,261] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,307] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.221.245\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,311] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,160] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,185] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,187] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,192] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mConfiguring Optimizer type: Adam with params: {'lr': 0.00015, 'betas': [0.9, 0.95], 'eps': 1e-08}\u001b[0m\n",
      "\u001b[34m> learning rate decay style: cosine\u001b[0m\n",
      "\u001b[34mDeepSpeed is enabled.\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,202] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+67e89b7, git-hash=67e89b7, git-branch=v2.0-stability\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,224] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,236] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34mHost: 10.0.236.88\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,244] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:50,245] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,193] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,215] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,238] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,254] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,279] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,292] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,301] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mHost: 10.0.229.58\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:11:50,307] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,193] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,240] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,245] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,289] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,292] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,295] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32mHost: 10.0.197.254\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,299] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,302] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,239] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,244] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32mHost: 10.0.226.42\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,249] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:11:50,250] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[32mBuilding extension module utils...\u001b[0m\n",
      "\u001b[32mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[36mBuilding extension module utils...\u001b[0m\n",
      "\u001b[36mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[33mBuilding extension module utils...\u001b[0m\n",
      "\u001b[33mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module utils...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,057] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,057] [INFO] [logging.py:77:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,057] [INFO] [logging.py:77:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [logging.py:77:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [stage_1_and_2.py:144:__init__] Reduce bucket size 1260000000\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [stage_1_and_2.py:145:__init__] Allgather bucket size 1260000000\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [stage_1_and_2.py:146:__init__] CPU Offload: False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:11:51,062] [INFO] [stage_1_and_2.py:147:__init__] Round robin gradient partitioning: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module utils...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[32mBuilding extension module utils...\u001b[0m\n",
      "\u001b[32mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[36m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[33m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[33m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.558298110961914 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.529215812683105 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.431750059127808 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.431818008422852 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.4311044216156 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.430539846420288 seconds\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.430460453033447 seconds\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.681915044784546 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.531012535095215 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.530214786529541 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.528850078582764 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.528971910476685 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.529086589813232 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.530668497085571 seconds\u001b[0m\n",
      "\u001b[35m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.735478639602661 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.52942681312561 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.533206224441528 secondsTime to load utils op: 14.52994441986084 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.733195066452026 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.632603645324707 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.63220477104187 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.631031036376953 seconds\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.725846767425537 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.628629922866821 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.632428646087646 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.628972291946411 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.627201080322266 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.627562046051025 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.627378940582275 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.73270583152771 seconds\u001b[0m\n",
      "\u001b[35m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.07570219039917 seconds\u001b[0m\n",
      "\u001b[32m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[32m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.43999457359314 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.230507850646973 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.230330467224121 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.230856895446777 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.431076049804688 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.330791473388672 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.329538822174072 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.33015251159668 seconds\u001b[0m\n",
      "\u001b[32m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.62539029121399 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.530578851699829 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.531425476074219 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.531073570251465 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.627825736999512 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.530556678771973 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.531255722045898 seconds\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 14.530405282974243 seconds\u001b[0m\n",
      "\u001b[36m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.49168848991394 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.42975640296936 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.331557989120483 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.33172607421875 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.332310914993286 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.432439088821411 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.432295322418213 seconds\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 14.431889533996582 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 14.430827140808105 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 14.730491638183594 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.928913593292236 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.928331136703491 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.031785249710083 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.931234121322632 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 14.929013013839722 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.02905797958374 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 15.028664827346802 seconds\u001b[0m\n",
      "\u001b[36mRank: 30 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 26 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[33mRank: 33 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 40 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 52 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 48 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 49 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 6 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 5 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 7 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 2 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 10 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 11 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 58 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 56 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 57 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 63 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 60 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 59 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 61 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 62 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[32mRank: 22 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 18 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 21 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 17 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 19 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 23 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 16 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[32mRank: 20 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 27 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 29 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 24 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 28 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 25 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[36mRank: 31 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[33mRank: 39 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 32 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 34 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 35 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 38 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 36 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[33mRank: 37 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 46 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 42 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 43 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 45 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 41 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 47 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 44 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 51 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 54 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 53 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 55 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[35mRank: 50 partition count [4, 4] and sizes[(415760384, False), (229376, False)]\u001b[0m\n",
      "\u001b[34mRank: 0 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 1 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 3 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[34mRank: 4 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 14 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 15 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 9 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 8 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 12 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mRank: 13 partition count [4, 4] and sizes[(415760384, False), (225280, False)]\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003848075866699219 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0004146099090576172 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003483295440673828 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003330707550048828 seconds\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00047135353088378906 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003383159637451172 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00033283233642578125 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0005085468292236328 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004673004150390625 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.00043964385986328125 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005419254302978516 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Time to load utils op: 0.0003948211669921875 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.000396728515625 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00040650367736816406 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003974437713623047 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,745] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,746] [INFO] [utils.py:830:see_memory_usage] MA 4.65 GB         Max_MA 5.42 GB         CA 4.67 GB         Max_CA 5 GB\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,746] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.79 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00043320655822753906 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003910064697265625 secondsTime to load utils op: 0.0005090236663818359 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00037932395935058594 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00034356117248535156 seconds\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Time to load utils op: 0.000400543212890625 seconds\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[35mLoading extension module utils...\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.00039005279541015625 seconds\u001b[0m\n",
      "\u001b[35mTime to load utils op: 0.0003821849822998047 seconds\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0004572868347167969 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...Loading extension module utils...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00038695335388183594 secondsTime to load utils op: 0.00038123130798339844 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00037288665771484375 secondsTime to load utils op: 0.00037169456481933594 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003790855407714844 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00037384033203125 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0004744529724121094 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003743171691894531 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003352165222167969 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.00034880638122558594 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0004336833953857422 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.000408172607421875 seconds\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003383159637451172 seconds\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0003941059112548828 seconds\u001b[0m\n",
      "\u001b[32mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[32mLoading extension module utils...\u001b[0m\n",
      "\u001b[32mTime to load utils op: 0.0005218982696533203 seconds\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003845691680908203 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003764629364013672 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...No modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...Loading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.00033926963806152344 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003643035888671875 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.00033283233642578125 seconds\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0003902912139892578 seconds\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.000423431396484375 seconds\u001b[0m\n",
      "\u001b[36mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[36mLoading extension module utils...\u001b[0m\n",
      "\u001b[36mTime to load utils op: 0.0005228519439697266 seconds\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0004291534423828125 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0004038810729980469 seconds\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003948211669921875 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.00033974647521972656 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.00038051605224609375 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003743171691894531 seconds\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0003955364227294922 seconds\u001b[0m\n",
      "\u001b[33mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[33mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[33mLoading extension module utils...\u001b[0m\n",
      "\u001b[33mTime to load utils op: 0.0005245208740234375 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003924369812011719 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00035953521728515625 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0003879070281982422 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.00033164024353027344 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.0003349781036376953 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004475116729736328 seconds\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0004181861877441406 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005016326904296875 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,841] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,842] [INFO] [utils.py:830:see_memory_usage] MA 7.75 GB         Max_MA 9.3 GB         CA 9.32 GB         Max_CA 9 GB\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,842] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.79 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,842] [INFO] [stage_1_and_2.py:520:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,926] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,927] [INFO] [utils.py:830:see_memory_usage] MA 7.75 GB         Max_MA 7.75 GB         CA 9.32 GB         Max_CA 9 GB\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,927] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.79 GB, percent = 4.2%\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f434244dac0>\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [logging.py:77:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [config.py:1022:print]   activation_checkpointing_config  {\u001b[0m\n",
      "\u001b[34m\"partition_activations\": false,\u001b[0m\n",
      "\u001b[34m\"contiguous_memory_optimization\": false,\u001b[0m\n",
      "\u001b[34m\"cpu_checkpointing\": false,\u001b[0m\n",
      "\u001b[34m\"number_checkpoints\": null,\u001b[0m\n",
      "\u001b[34m\"synchronize_checkpoint_boundary\": false,\u001b[0m\n",
      "\u001b[34m\"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [config.py:1022:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,929] [INFO] [config.py:1022:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   autotuning_config ............ {\u001b[0m\n",
      "\u001b[34m\"enabled\": false,\u001b[0m\n",
      "\u001b[34m\"start_step\": null,\u001b[0m\n",
      "\u001b[34m\"end_step\": null,\u001b[0m\n",
      "\u001b[34m\"metric_path\": null,\u001b[0m\n",
      "\u001b[34m\"arg_mappings\": null,\u001b[0m\n",
      "\u001b[34m\"metric\": \"throughput\",\u001b[0m\n",
      "\u001b[34m\"model_info\": null,\u001b[0m\n",
      "\u001b[34m\"results_dir\": \"autotuning_results\",\u001b[0m\n",
      "\u001b[34m\"exps_dir\": \"autotuning_exps\",\u001b[0m\n",
      "\u001b[34m\"overwrite\": true,\u001b[0m\n",
      "\u001b[34m\"fast\": true,\u001b[0m\n",
      "\u001b[34m\"start_profile_step\": 3,\u001b[0m\n",
      "\u001b[34m\"end_profile_step\": 5,\u001b[0m\n",
      "\u001b[34m\"tuner_type\": \"gridsearch\",\u001b[0m\n",
      "\u001b[34m\"tuner_early_stopping\": 5,\u001b[0m\n",
      "\u001b[34m\"tuner_num_trials\": 50,\u001b[0m\n",
      "\u001b[34m\"model_info_path\": null,\u001b[0m\n",
      "\u001b[34m\"mp_size\": 1,\u001b[0m\n",
      "\u001b[34m\"max_train_batch_size\": null,\u001b[0m\n",
      "\u001b[34m\"min_train_batch_size\": 1,\u001b[0m\n",
      "\u001b[34m\"max_train_micro_batch_size_per_gpu\": 1.024000e+03,\u001b[0m\n",
      "\u001b[34m\"min_train_micro_batch_size_per_gpu\": 1,\u001b[0m\n",
      "\u001b[34m\"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f434244dfd0>\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   flops_profiler_config ........ {\u001b[0m\n",
      "\u001b[34m\"enabled\": false,\u001b[0m\n",
      "\u001b[34m\"profile_step\": 1,\u001b[0m\n",
      "\u001b[34m\"module_depth\": -1,\u001b[0m\n",
      "\u001b[34m\"top_modules\": 1,\u001b[0m\n",
      "\u001b[34m\"detailed\": true,\u001b[0m\n",
      "\u001b[34m\"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 4096\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,930] [INFO] [config.py:1022:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   nebula_config ................ {\u001b[0m\n",
      "\u001b[34m\"enabled\": false,\u001b[0m\n",
      "\u001b[34m\"persistent_storage_path\": null,\u001b[0m\n",
      "\u001b[34m\"persistent_time_interval\": 100,\u001b[0m\n",
      "\u001b[34m\"num_of_version_in_retention\": 2,\u001b[0m\n",
      "\u001b[34m\"enable_nebula_load\": true,\u001b[0m\n",
      "\u001b[34m\"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   optimizer_name ............... adam\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   optimizer_params ............. {'lr': 0.00015, 'betas': [0.9, 0.95], 'eps': 1e-08}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   steps_per_print .............. 1\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   train_batch_size ............. 4\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=1260000000 allgather_partitions=True allgather_bucket_size=1260000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 1\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,931] [INFO] [config.py:1007:print_user_config]   json = {\u001b[0m\n",
      "\u001b[34m\"train_batch_size\": 4,\u001b[0m\n",
      "\u001b[34m\"train_micro_batch_size_per_gpu\": 1,\u001b[0m\n",
      "\u001b[34m\"optimizer\": {\u001b[0m\n",
      "\u001b[34m\"type\": \"Adam\",\u001b[0m\n",
      "\u001b[34m\"params\": {\u001b[0m\n",
      "\u001b[34m\"lr\": 0.00015,\u001b[0m\n",
      "\u001b[34m\"betas\": [0.9, 0.95],\u001b[0m\n",
      "\u001b[34m\"eps\": 1e-08\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m},\u001b[0m\n",
      "\u001b[34m\"fp16\": {\u001b[0m\n",
      "\u001b[34m\"fp16\": true,\u001b[0m\n",
      "\u001b[34m\"enabled\": true,\u001b[0m\n",
      "\u001b[34m\"loss_scale\": 0,\u001b[0m\n",
      "\u001b[34m\"loss_scale_window\": 1000,\u001b[0m\n",
      "\u001b[34m\"initial_scale_power\": 12,\u001b[0m\n",
      "\u001b[34m\"hysteresis\": 2,\u001b[0m\n",
      "\u001b[34m\"min_loss_scale\": 1\u001b[0m\n",
      "\u001b[34m},\u001b[0m\n",
      "\u001b[34m\"gradient_clipping\": 1.0,\u001b[0m\n",
      "\u001b[34m\"zero_optimization\": {\u001b[0m\n",
      "\u001b[34m\"stage\": 1,\u001b[0m\n",
      "\u001b[34m\"allgather_partitions\": true,\u001b[0m\n",
      "\u001b[34m\"allgather_bucket_size\": 1.260000e+09,\u001b[0m\n",
      "\u001b[34m\"overlap_comm\": true,\u001b[0m\n",
      "\u001b[34m\"reduce_scatter\": true,\u001b[0m\n",
      "\u001b[34m\"reduce_bucket_size\": 1.260000e+09,\u001b[0m\n",
      "\u001b[34m\"contiguous_gradients\": true,\u001b[0m\n",
      "\u001b[34m\"cpu_offload\": false\u001b[0m\n",
      "\u001b[34m},\u001b[0m\n",
      "\u001b[34m\"steps_per_print\": 1\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0003237724304199219 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:13,932] [INFO] [engine.py:88:__init__] CONFIG: micro_batches=1 micro_batch_size=1\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7NCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[36mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=33 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=34 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=35 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=36 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=37 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,865] [INFO] [engine.py:144:__init__] RANK=38 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,865] [INFO] [engine.py:144:__init__] RANK=39 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:15,865] [INFO] [engine.py:144:__init__] RANK=32 STAGE=1 LAYERS=19 [18, 37) STAGE_PARAMS=1663959040 (1663.959M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 3: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 2: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 0: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 7: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 5: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 1: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 6: 1663959040\u001b[0m\n",
      "\u001b[33m> number of parameters on model parallel rank 4: 1663959040\u001b[0m\n",
      "\u001b[33mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[34mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:16.584 algo-6:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:12:16.579 algo-7:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:12:16.727 algo-7:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=0 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=7 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=6 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=5 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=4 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=3 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,864] [INFO] [engine.py:144:__init__] RANK=2 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:15,865] [INFO] [engine.py:144:__init__] RANK=1 STAGE=0 LAYERS=18 [0, 18) STAGE_PARAMS=1663942656 (1663.943M) TOTAL_PARAMS=26623213568 (26623.214M) UNIQUE_PARAMS=26623213568 (26623.214M)\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 0: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 3: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 2: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 5: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 7: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 1: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 6: 1663942656\u001b[0m\n",
      "\u001b[34m> number of parameters on model parallel rank 4: 1663942656\u001b[0m\n",
      "\u001b[34m> total params: 26,623,213,568\u001b[0m\n",
      "\u001b[34m> building train, validation, and test datasets ...\u001b[0m\n",
      "\u001b[34mreading sizes...\u001b[0m\n",
      "\u001b[34mreading pointers...\u001b[0m\n",
      "\u001b[34mreading document index...\u001b[0m\n",
      "\u001b[34mcreating numpy buffer of mmap...\u001b[0m\n",
      "\u001b[34mcreating memory view of numpy buffer...\u001b[0m\n",
      "\u001b[34m> dataset split:\u001b[0m\n",
      "\u001b[34mtrain:\u001b[0m\n",
      "\u001b[34mdocument indices in [0, 1) total of 1 documents\u001b[0m\n",
      "\u001b[34mvalidation:\u001b[0m\n",
      "\u001b[34mdocument indices in [1, 1) total of 0 documents\u001b[0m\n",
      "\u001b[34mtest:\u001b[0m\n",
      "\u001b[34mdocument indices in [1, 1) total of 0 documents\u001b[0m\n",
      "\u001b[34m> loading doc-idx mapping from /opt/ml/input/data/train/enwik8/enwik8_text_document_train_indexmap_40ns_4096sl_1234s_doc_idx.npy\u001b[0m\n",
      "\u001b[34m> loading sample-idx mapping from /opt/ml/input/data/train/enwik8/enwik8_text_document_train_indexmap_40ns_4096sl_1234s_sample_idx.npy\u001b[0m\n",
      "\u001b[34m> loading shuffle-idx mapping from /opt/ml/input/data/train/enwik8/enwik8_text_document_train_indexmap_40ns_4096sl_1234s_shuffle_idx.npy\u001b[0m\n",
      "\u001b[34mloaded indexed file in 0.003 seconds\u001b[0m\n",
      "\u001b[34mtotal number of samples: 7084\u001b[0m\n",
      "\u001b[34mtotal number of epochs: 1\u001b[0m\n",
      "\u001b[34mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[34msetting training data start iteration to 0\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:16.573 algo-1:190 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:16.600 algo-1:190 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34mdone with setups ...\u001b[0m\n",
      "\u001b[34mtime (ms) | model and optimizer: 28542.41 | train/valid/test data iterators: 274.02\u001b[0m\n",
      "\u001b[34mtraining ...\u001b[0m\n",
      "\u001b[35mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:12:16.584 algo-2:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-04-27 04:12:16.794 algo-2:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:12:16.582 algo-8:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:12:16.762 algo-8:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[32mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:12:16.581 algo-3:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[32m[2023-04-27 04:12:16.771 algo-3:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[36mWARNING: shuffle index length (7082) is not equal to sample index length (7083)\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:12:16.576 algo-4:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[36m[2023-04-27 04:12:16.741 algo-4:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:16.585 algo-5:189 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[33m[2023-04-27 04:12:16.787 algo-5:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:16.773 algo-6:189 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:18,484] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:18,484] [INFO] [checkpointing.py:554:forward] ----Partition Activations True, CPU CHECKPOINTING False\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:18,484] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 32 total layers\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:18,484] [INFO] [checkpointing.py:560:forward] ----Synchronization True\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:18,484] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[32mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:29,352] [INFO] [logging.py:77:log_dist] [Rank 0] step=1, skipped=0, lr=[4.166666666666666e-08, 4.166666666666666e-08], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 1 loss: 12.5466 iter time (s): 13.720 samples/sec: 0.292\u001b[0m\n",
      "\u001b[34msamples/sec: 0.291 | iteration        1/      10 | elapsed time per iteration (ms): 13724.1 | learning rate: 4.167E-08 | approx flops per GPU: 4.2TFLOPS | lm_loss: 1.254662E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mafter 1 iterations memory (MB) | allocated: 7966.37060546875 | max allocated: 13480.0888671875 | reserved: 18928.0 | max reserved: 18928.0\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:32,257] [INFO] [logging.py:77:log_dist] [Rank 0] step=2, skipped=0, lr=[8.333333333333333e-08, 8.333333333333333e-08], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 2 loss: 12.5746 iter time (s): 2.580 samples/sec: 1.550\u001b[0m\n",
      "\u001b[34msamples/sec: 1.549 | iteration        2/      10 | elapsed time per iteration (ms): 2582.0 | learning rate: 8.333E-08 | approx flops per GPU: 22.2TFLOPS | lm_loss: 1.257463E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:34,840] [INFO] [logging.py:77:log_dist] [Rank 0] step=3, skipped=0, lr=[1.25e-07, 1.25e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 3 loss: 12.4993 iter time (s): 2.593 samples/sec: 1.543\u001b[0m\n",
      "\u001b[34msamples/sec: 1.542 | iteration        3/      10 | elapsed time per iteration (ms): 2594.0 | learning rate: 1.250E-07 | approx flops per GPU: 22.1TFLOPS | lm_loss: 1.249932E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:37,427] [INFO] [logging.py:77:log_dist] [Rank 0] step=4, skipped=0, lr=[1.6666666666666665e-07, 1.6666666666666665e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 4 loss: 12.4598 iter time (s): 2.585 samples/sec: 1.548\u001b[0m\n",
      "\u001b[34msamples/sec: 1.547 | iteration        4/      10 | elapsed time per iteration (ms): 2585.9 | learning rate: 1.667E-07 | approx flops per GPU: 22.2TFLOPS | lm_loss: 1.245977E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:40,025] [INFO] [logging.py:77:log_dist] [Rank 0] step=5, skipped=0, lr=[2.083333333333333e-07, 2.083333333333333e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 5 loss: 12.5176 iter time (s): 2.588 samples/sec: 1.546\u001b[0m\n",
      "\u001b[34msamples/sec: 1.545 | iteration        5/      10 | elapsed time per iteration (ms): 2588.8 | learning rate: 2.083E-07 | approx flops per GPU: 22.2TFLOPS | lm_loss: 1.251764E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:42,600] [INFO] [logging.py:77:log_dist] [Rank 0] step=6, skipped=0, lr=[2.5e-07, 2.5e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 6 loss: 11.9254 iter time (s): 2.585 samples/sec: 1.548\u001b[0m\n",
      "\u001b[34msamples/sec: 1.547 | iteration        6/      10 | elapsed time per iteration (ms): 2585.8 | learning rate: 2.500E-07 | approx flops per GPU: 22.2TFLOPS | lm_loss: 1.192539E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:45,190] [INFO] [logging.py:77:log_dist] [Rank 0] step=7, skipped=0, lr=[2.9166666666666664e-07, 2.9166666666666664e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 7 loss: 11.0721 iter time (s): 2.591 samples/sec: 1.544\u001b[0m\n",
      "\u001b[34msamples/sec: 1.543 | iteration        7/      10 | elapsed time per iteration (ms): 2591.8 | learning rate: 2.917E-07 | approx flops per GPU: 22.1TFLOPS | lm_loss: 1.107210E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:47,760] [INFO] [logging.py:77:log_dist] [Rank 0] step=8, skipped=0, lr=[3.333333333333333e-07, 3.333333333333333e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 8 loss: 10.7025 iter time (s): 2.555 samples/sec: 1.565\u001b[0m\n",
      "\u001b[34msamples/sec: 1.565 | iteration        8/      10 | elapsed time per iteration (ms): 2556.5 | learning rate: 3.333E-07 | approx flops per GPU: 22.4TFLOPS | lm_loss: 1.070248E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:50,328] [INFO] [logging.py:77:log_dist] [Rank 0] step=9, skipped=0, lr=[3.7499999999999996e-07, 3.7499999999999996e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 9 loss: 11.9616 iter time (s): 2.569 samples/sec: 1.557\u001b[0m\n",
      "\u001b[34msamples/sec: 1.556 | iteration        9/      10 | elapsed time per iteration (ms): 2570.1 | learning rate: 3.750E-07 | approx flops per GPU: 22.3TFLOPS | lm_loss: 1.196161E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[34m[2023-04-27 04:12:52,897] [INFO] [logging.py:77:log_dist] [Rank 0] step=10, skipped=0, lr=[4.166666666666666e-07, 4.166666666666666e-07], mom=[[0.9, 0.95], [0.9, 0.95]]\u001b[0m\n",
      "\u001b[34msteps: 10 loss: 12.0740 iter time (s): 2.568 samples/sec: 1.558\u001b[0m\n",
      "\u001b[34msamples/sec: 1.557 | iteration       10/      10 | elapsed time per iteration (ms): 2568.9 | learning rate: 4.167E-07 | approx flops per GPU: 22.3TFLOPS | lm_loss: 1.207402E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 |\u001b[0m\n",
      "\u001b[34mtime (ms)\u001b[0m\n",
      "\u001b[33m2023-04-27 04:12:57,359 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[33m2023-04-27 04:12:57,359 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[33m2023-04-27 04:12:57,360 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m2023-04-27 04:12:57,366 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:12:57,367 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:12:57,367 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2023-04-27 04:12:57,362 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:12:57,362 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:12:57,363 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m2023-04-27 04:12:57,360 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:12:57,360 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-27 04:12:57,360 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m2023-04-27 04:12:57,363 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:12:57,363 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:12:57,364 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2023-04-27 04:12:57,369 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:12:57,369 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-04-27 04:12:57,369 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[32m2023-04-27 04:12:57,356 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:12:57,356 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[32m2023-04-27 04:12:57,356 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[36m2023-04-27 04:12:57,362 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[36m2023-04-27 04:12:57,363 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[36m2023-04-27 04:12:57,363 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-04-27 04:13:15 Uploading - Uploading generated training model\n",
      "2023-04-27 04:13:15 Completed - Resource retained for reuse\n",
      "Training seconds: 944\n",
      "Billable seconds: 944\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.instance_group import InstanceGroup\n",
    "\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"mds-gpt-neox\",\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"gpt-neox/run_torchrun.py\",\n",
    "    role=role,\n",
    "    image_uri=\"570106654206.dkr.ecr.us-east-1.amazonaws.com/megatron-deepspeed:pt1.13.1-deeperspeed-0.8.3-v1\",\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=8,\n",
    "    # For training with p3dn instance use - ml.p3dn.24xlarge, with p4dn instance use - ml.p4d.24xlarge\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    # Training using SMDataParallel Distributed Training Framework\n",
    "    debugger_hook_config=False,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    ")\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
